From 1021883d5a0ec2a8bd6cc38cb9d5ad4e8b85b019 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Wed, 4 Oct 2023 15:43:53 +0000
Subject: [PATCH 01/26] Change Makefile to include HOMA

---
 drivers/nvme/target/Makefile | 1 +
 1 file changed, 1 insertion(+)

diff --git a/drivers/nvme/target/Makefile b/drivers/nvme/target/Makefile
index c66820102493..89419cb26889 100644
--- a/drivers/nvme/target/Makefile
+++ b/drivers/nvme/target/Makefile
@@ -1,6 +1,7 @@
 # SPDX-License-Identifier: GPL-2.0
 
 ccflags-y				+= -I$(src)
+ccflags-y				+= -I/home/ubuntu/HomaModule/
 
 obj-$(CONFIG_NVME_TARGET)		+= nvmet.o
 obj-$(CONFIG_NVME_TARGET_LOOP)		+= nvme-loop.o
-- 
2.37.2


From 3c6dd8d49d5692541b4322f6474cc3f787f8405f Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Wed, 4 Oct 2023 16:25:00 +0000
Subject: [PATCH 02/26] Added Initital Changes to set up main HOMA socket

---
 drivers/nvme/target/tcp.c | 80 +++++++++++++++++++++------------------
 1 file changed, 43 insertions(+), 37 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 868aa4de2e4c..52b3b1fcf8c3 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -17,6 +17,7 @@
 #include <trace/events/sock.h>
 
 #include "nvmet.h"
+#include "homa.h"
 
 #define NVMET_TCP_DEF_INLINE_DATA_SIZE	(4 * PAGE_SIZE)
 
@@ -178,6 +179,7 @@ struct nvmet_tcp_queue {
 
 struct nvmet_tcp_port {
 	struct socket		*sock;
+	struct homa_set_buf_args homa_buf_args;
 	struct work_struct	accept_work;
 	struct nvmet_port	*nport;
 	struct sockaddr_storage addr;
@@ -1571,8 +1573,8 @@ static int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)
 	struct socket *sock = queue->sock;
 	struct inet_sock *inet = inet_sk(sock->sk);
 	int ret;
-
-	ret = kernel_getsockname(sock,
+	//TODO: COME HERE AND CLEAR UP LARGE COMMENTED OUT BLOCKS
+	/*ret = kernel_getsockname(sock,
 		(struct sockaddr *)&queue->sockaddr);
 	if (ret < 0)
 		return ret;
@@ -1580,42 +1582,34 @@ static int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)
 	ret = kernel_getpeername(sock,
 		(struct sockaddr *)&queue->sockaddr_peer);
 	if (ret < 0)
-		return ret;
+		return ret;*/
 
 	/*
 	 * Cleanup whatever is sitting in the TCP transmit queue on socket
 	 * close. This is done to prevent stale data from being sent should
 	 * the network connection be restored before TCP times out.
 	 */
-	sock_no_linger(sock->sk);
+	/*sock_no_linger(sock->sk);
 
 	if (so_priority > 0)
 		sock_set_priority(sock->sk, so_priority);
 
-	/* Set socket type of service */
+	 Set socket type of service 
 	if (inet->rcv_tos > 0)
-		ip_sock_set_tos(sock->sk, inet->rcv_tos);
+		ip_sock_set_tos(sock->sk, inet->rcv_tos);*/
 
 	ret = 0;
 	write_lock_bh(&sock->sk->sk_callback_lock);
-	if (sock->sk->sk_state != TCP_ESTABLISHED) {
-		/*
-		 * If the socket is already closing, don't even start
-		 * consuming it
-		 */
-		ret = -ENOTCONN;
-	} else {
-		sock->sk->sk_user_data = queue;
-		queue->data_ready = sock->sk->sk_data_ready;
-		sock->sk->sk_data_ready = nvmet_tcp_data_ready;
-		queue->state_change = sock->sk->sk_state_change;
-		sock->sk->sk_state_change = nvmet_tcp_state_change;
-		queue->write_space = sock->sk->sk_write_space;
-		sock->sk->sk_write_space = nvmet_tcp_write_space;
-		if (idle_poll_period_usecs)
-			nvmet_tcp_arm_queue_deadline(queue);
-		queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
-	}
+	sock->sk->sk_user_data = queue;
+	queue->data_ready = sock->sk->sk_data_ready;
+	sock->sk->sk_data_ready = nvmet_tcp_data_ready;
+	queue->state_change = sock->sk->sk_state_change;
+	sock->sk->sk_state_change = nvmet_tcp_state_change;
+	queue->write_space = sock->sk->sk_write_space;
+	sock->sk->sk_write_space = nvmet_tcp_write_space;
+	if (idle_poll_period_usecs)
+		nvmet_tcp_arm_queue_deadline(queue);
+	//queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
 	write_unlock_bh(&sock->sk->sk_callback_lock);
 
 	return ret;
@@ -1724,11 +1718,13 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 {
 	struct nvmet_tcp_port *port;
 	__kernel_sa_family_t af;
+	int bufsize = 64 * HOMA_BPAGE_SIZE;
 	int ret;
 
 	port = kzalloc(sizeof(*port), GFP_KERNEL);
 	if (!port)
 		return -ENOMEM;
+	char *server_homa_buf_region = kzalloc(bufsize,GFP_KERNEL);
 
 	switch (nport->disc_addr.adrfam) {
 	case NVMF_ADDR_FAMILY_IP4:
@@ -1753,24 +1749,29 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 	}
 
 	port->nport = nport;
-	INIT_WORK(&port->accept_work, nvmet_tcp_accept_work);
+	//INIT_WORK(&port->accept_work, nvmet_tcp_accept_work);
 	if (port->nport->inline_data_size < 0)
 		port->nport->inline_data_size = NVMET_TCP_DEF_INLINE_DATA_SIZE;
 
-	ret = sock_create(port->addr.ss_family, SOCK_STREAM,
-				IPPROTO_TCP, &port->sock);
+	ret = sock_create(port->addr.ss_family, SOCK_DGRAM,
+				IPPROTO_HOMA, &port->sock);
 	if (ret) {
 		pr_err("failed to create a socket\n");
 		goto err_port;
 	}
 
+	port->homa_buf_args.length = bufsize;
+	port->homa_buf_args.start = server_homa_buf_region;
 	port->sock->sk->sk_user_data = port;
 	port->data_ready = port->sock->sk->sk_data_ready;
-	port->sock->sk->sk_data_ready = nvmet_tcp_listen_data_ready;
-	sock_set_reuseaddr(port->sock->sk);
-	tcp_sock_set_nodelay(port->sock->sk);
-	if (so_priority > 0)
-		sock_set_priority(port->sock->sk, so_priority);
+	ret = port->sock->ops->setsockopt(
+		port->sock, IPPROTO_HOMA, SO_HOMA_SET_BUF,KERNEL_SOCKPTR(&(port->homa_buf_args)),
+		sizeof(port->homa_buf_args));
+	if (ret < 0){
+		pr_err("%d, %s \n", ret, "Cannot set sockopt for HOMA in kernel");
+	}
+	/*if (so_priority > 0)
+		sock_set_priority(port->sock->sk, so_priority);*/
 
 	ret = kernel_bind(port->sock, (struct sockaddr *)&port->addr,
 			sizeof(port->addr));
@@ -1779,21 +1780,25 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 		goto err_sock;
 	}
 
-	ret = kernel_listen(port->sock, 128);
-	if (ret) {
-		pr_err("failed to listen %d on port sock\n", ret);
-		goto err_sock;
-	}
 
 	nport->priv = port;
 	pr_info("enabling port %d (%pISpc)\n",
 		le16_to_cpu(nport->disc_addr.portid), &port->addr);
+	
+	ret = nvmet_tcp_alloc_queue(port, port->sock);
+	if (ret){
+		pr_err("Failed to allocate queue\n");
+		sock_release(port->sock);
+		kfree(server_homa_buf_region);
+	}
 
 	return 0;
 
 err_sock:
 	sock_release(port->sock);
+	kfree(server_homa_buf_region);
 err_port:
+	kfree(server_homa_buf_region);
 	kfree(port);
 	return ret;
 }
@@ -1825,6 +1830,7 @@ static void nvmet_tcp_remove_port(struct nvmet_port *nport)
 	nvmet_tcp_destroy_port_queues(port);
 
 	sock_release(port->sock);
+	kfree(port->homa_buf_args.start);
 	kfree(port);
 }
 
-- 
2.37.2


From fa8f89ab01c7e57ff5e2724918770633e4929064 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:02:26 +0000
Subject: [PATCH 03/26] Modify Receive Function to Use HOMA

---
 drivers/nvme/target/tcp.c | 37 +++++++++++++++++++++++++++++++++++--
 1 file changed, 35 insertions(+), 2 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 52b3b1fcf8c3..01fd3d7f15f1 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -1235,18 +1235,50 @@ static int nvmet_tcp_try_recv_ddgst(struct nvmet_tcp_queue *queue)
 static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 {
 	int result = 0;
+	int len;
+	sockaddr_in_union source;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct homa_recvmsg_args *homa_recv_args;
+
+	homa_recv_args = kzalloc(sizeof(*homa_recv_args), GFP_KERNEL);
+	homa_recv_args->flags |= HOMA_RECVMSG_REQUEST; 
+	homa_recv_args->flags |= HOMA_RECVMSG_NONBLOCKING;
+	homa_recv_args->num_bpages = 0;
+	msg.msg_control = homa_recv_args;
+	msg.msg_controllen = sizeof(*homa_recv_args);
+	msg.msg_name = &(source);
+	msg.msg_namelen = sizeof(source);
+	len = sock_recvmsg(queue->sock, &msg, msg.msg_flags);
+	if (len == -EAGAIN) {
+		pr_info("sock_recvmsg returned EAGAIN\n");
+		return 0;
+	}
+	if (unlikely(len < 0)){
+		pr_info("sock_recvmsg error returned %d\n", len);
+		return len;
+	}
+	pr_info("length of message received is : %d\n", len);
+
+	queue->response_state.response_id = homa_recv_args->id;
+	memcpy(&queue->response_state.source_addr, msg.msg_name, sizeof(sockaddr_in_union));
 
 	if (unlikely(queue->rcv_state == NVMET_TCP_RECV_ERR))
 		return 0;
 
 	if (queue->rcv_state == NVMET_TCP_RECV_PDU) {
-		result = nvmet_tcp_try_recv_pdu(queue);
+		pr_info("Trying to received pdu\n");
+		/*we can use queue->offset as our offset tracker into the homa ppol*/
+		/*^ Addendum to above comment this does not work lol, cause copy_to location
+		is queue->pdu + queue_offset so we would kinda be fokken up here*/
+		/*Wait nvm im a genois*/
+		result = nvmet_tcp_try_recv_pdu(queue, homa_recv_args, &queue->offset);
 		if (result != 0)
 			goto done_recv;
 	}
 
 	if (queue->rcv_state == NVMET_TCP_RECV_DATA) {
-		result = nvmet_tcp_try_recv_data(queue);
+		pr_info("Trying to received data\n");
+		result = nvmet_tcp_try_recv_data(homa_recv_args, queue, &queue->offset);
 		if (result != 0)
 			goto done_recv;
 	}
@@ -1258,6 +1290,7 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 	}
 
 done_recv:
+	kfree(homa_recv_args);
 	if (result < 0) {
 		if (result == -EAGAIN)
 			return 0;
-- 
2.37.2


From 51a6743f5abd3631c9c2fc8a529ea3f74e289f07 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:08:52 +0000
Subject: [PATCH 04/26] Add Functions to Copy From HOMA pool

---
 drivers/nvme/target/tcp.c | 70 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 70 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 01fd3d7f15f1..df22c8c5a2cb 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -195,6 +195,76 @@ static const struct nvmet_fabrics_ops nvmet_tcp_ops;
 static void nvmet_tcp_free_cmd(struct nvmet_tcp_cmd *c);
 static void nvmet_tcp_free_cmd_buffers(struct nvmet_tcp_cmd *cmd);
 
+/* copy_from_homa_pool - copy data from the homa buffer pool to another buffer
+   @homa_recv_args: The homa_recvmsg_args struct that contains the information of our message in the
+   homa buffer pool
+   @len_to_copy: The length of the data to copy
+   @offset: The offset into the homa buffer pool to start copying from, this is updated in the
+   function as we iterate through the homa buffer pool
+   @queue: The queue that the homa buffer pool is associated with
+   @copy_to: The memory location to copy the data to
+
+   We just provide copying from the homa buffer pool, along with an offset that will allow us to 
+   partially copy from the buffer pool and start from the appropiate point in the pool again. It's
+   the users responsibilty to choose an appropriate offset variable.
+*/
+static int copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, int len_to_copy, unsigned int *offset,
+	struct nvmet_tcp_queue *queue, void *copy_to){
+
+	pr_info("HOMA response id : %llu\n", (unsigned long long) homa_recv_args->id);
+	pr_info("HOMA completion cookie : %llu\n", (unsigned long long)homa_recv_args->completion_cookie);
+	pr_info("The offset we are working with is %d: \n", *offset);
+
+	int data_offset = *offset & (HOMA_BPAGE_SIZE - 1);
+	int page_offset = (*offset / HOMA_BPAGE_SIZE);	
+
+	/*Set the iterating variable to the remaiing number of pages to go through*/
+	for(uint32_t i = (page_offset); (i < homa_recv_args->num_bpages || len_to_copy > 0); i++){
+	size_t len = ((len_to_copy > HOMA_BPAGE_SIZE) ? 
+	HOMA_BPAGE_SIZE :
+		len_to_copy);
+		//ohhhhh, wait data_offset should only appy for the first page, after that we should start from 0. Sheeeeeeee.
+		memcpy(copy_to+ *offset, queue->port->homa_buf_args.start+ homa_recv_args->bpage_offsets[i] + data_offset, len);
+		*offset += len;
+		len_to_copy -= len;
+	}
+
+
+
+	return 0;
+}
+/* copy_from_homa_pool - copy data from the homa buffer pool to another buffer
+   @homa_recv_args: The homa_recvmsg_args struct that contains the information of our message in the
+   homa buffer pool
+   @len_to_copy: The length of the data to copy
+   @offset: The offset into the homa buffer pool to start copying from, this is updated in the
+   function as we iterate through the homa buffer pool
+   @queue: The queue that the homa buffer pool is associated with
+   @iter: The iterator that copies the data to the intended location
+
+   We just provide copying from the homa buffer pool, along with an offset that will allow us to 
+   partially copy from the buffer pool and start from the appropiate point in the pool again. It's
+   the users responsibilty to choose an appropriate offset variable.
+*/
+static int iter_copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, int len_to_copy, unsigned int *offset,
+	struct nvmet_tcp_queue *queue, struct iov_iter *iter){
+
+	int data_offset = *offset & (HOMA_BPAGE_SIZE - 1);
+	int page_offset = (*offset / HOMA_BPAGE_SIZE);	
+
+	/*Set the iterating variable to the remaiing number of pages to go through*/
+	for(uint32_t i = (page_offset); (i < homa_recv_args->num_bpages || len_to_copy > 0); i++){
+	size_t len = ((len_to_copy > HOMA_BPAGE_SIZE) ? 
+	HOMA_BPAGE_SIZE :
+		len_to_copy);
+		copy_to_iter(queue->port->homa_buf_args.start+ homa_recv_args->bpage_offsets[i] + data_offset, len, iter);
+		*offset += len;
+		len_to_copy -= len;
+	}
+
+	return 0;
+}
+
 static inline u16 nvmet_tcp_cmd_tag(struct nvmet_tcp_queue *queue,
 		struct nvmet_tcp_cmd *cmd)
 {
-- 
2.37.2


From 76f2e463d843b548e9a7f068c44d8d05c6743bfd Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:12:55 +0000
Subject: [PATCH 05/26] Changes to Allow Accepting and Responding to ICREQ

---
 drivers/nvme/target/tcp.c | 45 +++++++++++++++++++++++----------------
 1 file changed, 27 insertions(+), 18 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index df22c8c5a2cb..f27e3359590a 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -938,6 +938,7 @@ static int nvmet_tcp_handle_icreq(struct nvmet_tcp_queue *queue)
 	struct nvme_tcp_icresp_pdu *icresp = &queue->pdu.icresp;
 	struct msghdr msg = {};
 	struct kvec iov;
+	struct homa_sendmsg_args homa_send_args;
 	int ret;
 
 	if (le32_to_cpu(icreq->hdr.plen) != sizeof(struct nvme_tcp_icreq_pdu)) {
@@ -978,6 +979,14 @@ static int nvmet_tcp_handle_icreq(struct nvmet_tcp_queue *queue)
 	if (queue->data_digest)
 		icresp->digest |= NVME_TCP_DATA_DIGEST_ENABLE;
 
+	memset(&homa_send_args, 0, sizeof(homa_send_args));
+	homa_send_args.id  = queue->response_state.response_id;
+
+	msg.msg_name = &(queue->response_state.source_addr);
+	msg.msg_namelen = sizeof((queue->response_state.source_addr));
+	msg.msg_control = &homa_send_args;
+	msg.msg_controllen = sizeof(homa_send_args);
+	msg.msg_control_is_user = false;
 	iov.iov_base = icresp;
 	iov.iov_len = sizeof(*icresp);
 	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
@@ -1074,6 +1083,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 			nvmet_tcp_fatal_error(queue);
 			return -EPROTO;
 		}
+		pr_info("Handling ICreq");
 		return nvmet_tcp_handle_icreq(queue);
 	}
 
@@ -1171,27 +1181,20 @@ static inline bool nvmet_tcp_pdu_valid(u8 type)
 	return false;
 }
 
-static int nvmet_tcp_try_recv_pdu(struct nvmet_tcp_queue *queue)
+static int nvmet_tcp_try_recv_pdu(struct nvmet_tcp_queue *queue,
+struct homa_recvmsg_args *homa_recv_args, int *offset)
 {
 	struct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;
-	int len;
-	struct kvec iov;
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	int len = (int) sizeof(struct nvme_tcp_hdr);
 
-recv:
-	iov.iov_base = (void *)&queue->pdu + queue->offset;
-	iov.iov_len = queue->left;
-	len = kernel_recvmsg(queue->sock, &msg, &iov, 1,
-			iov.iov_len, msg.msg_flags);
-	if (unlikely(len < 0))
-		return len;
+	pr_info("Copying common header!, num bits: %d\n", len);
+	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
 
-	queue->offset += len;
 	queue->left -= len;
-	if (queue->left)
-		return -EAGAIN;
 
-	if (queue->offset == sizeof(struct nvme_tcp_hdr)) {
+
+
+	//if (queue->offset == sizeof(struct nvme_tcp_hdr)) 
 		u8 hdgst = nvmet_tcp_hdgst_len(queue);
 
 		if (unlikely(!nvmet_tcp_pdu_valid(hdr->type))) {
@@ -1204,10 +1207,16 @@ static int nvmet_tcp_try_recv_pdu(struct nvmet_tcp_queue *queue)
 			pr_err("pdu %d bad hlen %d\n", hdr->type, hdr->hlen);
 			return -EIO;
 		}
+	pr_info("Header type: %d\n", hdr->type);
+	pr_info("Header len: %d\n", hdr->hlen);
 
-		queue->left = hdr->hlen - queue->offset + hdgst;
-		goto recv;
-	}
+	/*queue->left = hdr->hlen - queue->offset + hdgst;
+	goto recv;*/
+
+	len = hdr->hlen - queue->offset;;
+	pr_info("Copying protocol specific header!, num bits: %d\n", len);
+	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
+	queue->left -= len;
 
 	if (queue->hdr_digest &&
 	    nvmet_tcp_verify_hdgst(queue, &queue->pdu, hdr->hlen)) {
-- 
2.37.2


From d359c16d0892bd2e9227640a0ecb8e972d1695b0 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:42:15 +0000
Subject: [PATCH 06/26] Enable Homa on the Data Receiving Path

---
 drivers/nvme/target/tcp.c | 18 +++++++-----------
 1 file changed, 7 insertions(+), 11 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index f27e3359590a..84929ec6ff6b 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -1243,20 +1243,16 @@ static void nvmet_tcp_prep_recv_ddgst(struct nvmet_tcp_cmd *cmd)
 	queue->rcv_state = NVMET_TCP_RECV_DDGST;
 }
 
-static int nvmet_tcp_try_recv_data(struct nvmet_tcp_queue *queue)
+static int nvmet_tcp_try_recv_data(struct homa_recvmsg_args *homa_recv_args, struct nvmet_tcp_queue *queue, int *offset)
 {
 	struct nvmet_tcp_cmd  *cmd = queue->cmd;
-	int ret;
 
-	while (msg_data_left(&cmd->recv_msg)) {
-		ret = sock_recvmsg(cmd->queue->sock, &cmd->recv_msg,
-			cmd->recv_msg.msg_flags);
-		if (ret <= 0)
-			return ret;
-
-		cmd->pdu_recv += ret;
-		cmd->rbytes_done += ret;
-	}
+	pr_info("Receiving data in this recvmsg\n");
+	int len = cmd->recv_msg.msg_iter.count;
+	/*Can probably remove the msghdr ting we got goin on*/
+	iter_copy_from_homa_pool(homa_recv_args, len, offset, queue, &cmd->recv_msg.msg_iter);
+	cmd->pdu_recv += len;
+	cmd->rbytes_done += len;
 
 	if (queue->data_digest) {
 		nvmet_tcp_prep_recv_ddgst(cmd);
-- 
2.37.2


From 0bbf5d1c0de7c8245b45b23e4a59ee736e4addc7 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:44:05 +0000
Subject: [PATCH 07/26] Set Up Bvecs and Info Messages for Send Path

---
 drivers/nvme/target/tcp.c | 17 +++++++++++++----
 1 file changed, 13 insertions(+), 4 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 84929ec6ff6b..2b8bcaf0ef77 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -831,20 +831,26 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 		if (unlikely(!cmd))
 			return 0;
 	}
+	pr_info("Num of sg pages is : %d\n", cmd->req.sg_cnt);
+	struct bio_vec *data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
+	, GFP_KERNEL);
 
 	if (cmd->state == NVMET_TCP_SEND_DATA_PDU) {
-		ret = nvmet_try_send_data_pdu(cmd);
+		pr_info("Sending data pdu \n");
+		ret = nvmet_try_send_data_pdu(cmd, data_bvecs);
 		if (ret <= 0)
 			goto done_send;
 	}
 
 	if (cmd->state == NVMET_TCP_SEND_DATA) {
-		ret = nvmet_try_send_data(cmd, last_in_batch);
+		pr_info("Sending data \n");
+		ret = nvmet_try_send_data(cmd, data_bvecs,last_in_batch);
 		if (ret <= 0)
 			goto done_send;
 	}
 
 	if (cmd->state == NVMET_TCP_SEND_DDGST) {
+		pr_info("Sending ddgst \n");
 		ret = nvmet_try_send_ddgst(cmd, last_in_batch);
 		if (ret <= 0)
 			goto done_send;
@@ -856,10 +862,13 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 			goto done_send;
 	}
 
-	if (cmd->state == NVMET_TCP_SEND_RESPONSE)
-		ret = nvmet_try_send_response(cmd, last_in_batch);
+	if (cmd->state == NVMET_TCP_SEND_RESPONSE){
+		pr_info("Sending response \n");
+		ret = nvmet_try_send_response(cmd, data_bvecs,last_in_batch);
+	}
 
 done_send:
+	kfree(data_bvecs);
 	if (ret < 0) {
 		if (ret == -EAGAIN)
 			return 0;
-- 
2.37.2


From 2124e423e24d497e5ca4fe3e4dd17d8da0b167bf Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:47:10 +0000
Subject: [PATCH 08/26] Set Up Stateful HOMA Args on the Queue and Command
 struct

---
 drivers/nvme/target/tcp.c | 11 +++++++++++
 1 file changed, 11 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 2b8bcaf0ef77..05f5d24f47ea 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -85,6 +85,15 @@ enum nvmet_tcp_recv_state {
 	NVMET_TCP_RECV_DDGST,
 	NVMET_TCP_RECV_ERR,
 };
+struct nvmet_homa_socket_response_state {
+	/*source addr store the sockaddr val so it is not a pointe here
+	this was done because the sockaddr val we get is from msghdr. Initially I had the memory 
+	allocated statically however, the information would have to be passed over to nvmet_tcp_cmd
+	 struct which would use it in the send path. The info would be long gone by then. I could 
+	 have dynamically allocated the */
+	sockaddr_in_union source_addr;
+	uint64_t response_id;
+};
 
 enum {
 	NVMET_TCP_F_INIT_FAILED = (1 << 0),
@@ -92,6 +101,7 @@ enum {
 
 struct nvmet_tcp_cmd {
 	struct nvmet_tcp_queue		*queue;
+	struct nvmet_homa_socket_response_state response_state;
 	struct nvmet_req		req;
 
 	struct nvme_tcp_cmd_pdu		*cmd_pdu;
@@ -146,6 +156,7 @@ struct nvmet_tcp_queue {
 	/* recv state */
 	int			offset;
 	int			left;
+	struct nvmet_homa_socket_response_state response_state;
 	enum nvmet_tcp_recv_state rcv_state;
 	struct nvmet_tcp_cmd	*cmd;
 	union nvme_tcp_pdu	pdu;
-- 
2.37.2


From 83227a90a300bf88218955d610f01b616528506b Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:48:24 +0000
Subject: [PATCH 09/26] Copy over HOMA state from queue to cmd

---
 drivers/nvme/target/tcp.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 05f5d24f47ea..46d422a8dbb1 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -322,6 +322,10 @@ nvmet_tcp_get_cmd(struct nvmet_tcp_queue *queue)
 		return NULL;
 	list_del_init(&cmd->entry);
 
+	/*So basically we copy over the response state into cmd because those variables in the 
+	queue struct will get rewritten once recvmsg is called again*/
+	cmd->response_state.source_addr = queue->response_state.source_addr;
+	cmd->response_state.response_id = queue->response_state.response_id;
 	cmd->rbytes_done = cmd->wbytes_done = 0;
 	cmd->pdu_len = 0;
 	cmd->pdu_recv = 0;
-- 
2.37.2


From 2941f002fd08efbae1aa7317ec5ddaed94ea0370 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:50:24 +0000
Subject: [PATCH 10/26] Add Info Messaging on the Process Received PDU path

---
 drivers/nvme/target/tcp.c | 8 ++++++++
 1 file changed, 8 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 46d422a8dbb1..972c16a2e49f 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -1125,6 +1125,10 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 		return 0;
 	}
 
+	pr_info("SOme sanity checks!!!\n");
+	pr_info("COmmand identifier is %d\n", nvme_cmd->fabrics.command_id);
+	pr_info("COmmnad fctype is %d\n", nvme_cmd->fabrics.fctype);
+
 	queue->cmd = nvmet_tcp_get_cmd(queue);
 	if (unlikely(!queue->cmd)) {
 		/* This should never happen */
@@ -1137,6 +1141,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 
 	req = &queue->cmd->req;
 	memcpy(req->cmd, nvme_cmd, sizeof(*nvme_cmd));
+	pr_info("succesfully copied nvme_cmd\n");
 
 	if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
 			&queue->nvme_sq, &nvmet_tcp_ops))) {
@@ -1149,7 +1154,9 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 		return 0;
 	}
 
+	pr_info("beginning to map nvme_cmd\n");
 	ret = nvmet_tcp_map_data(queue->cmd);
+	pr_info("succesfully mapped nvme_cmd\n");
 	if (unlikely(ret)) {
 		pr_err("queue %d: failed to map data\n", queue->idx);
 		if (nvmet_tcp_has_inline_data(queue->cmd))
@@ -1172,6 +1179,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 	}
 
 	queue->cmd->req.execute(&queue->cmd->req);
+	pr_info("succhesfully executed nvme command\n");
 out:
 	nvmet_prepare_receive_pdu(queue);
 	return ret;
-- 
2.37.2


From 9786928445aff00d968f85549139e6a1653158cb Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:52:07 +0000
Subject: [PATCH 11/26] Format Send Response to Use HOMA

---
 drivers/nvme/target/tcp.c | 41 ++++++++++++++++++++++++++++-----------
 1 file changed, 30 insertions(+), 11 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 972c16a2e49f..07f2099789f7 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -741,30 +741,49 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 
 }
 
-static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd,
+static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		bool last_in_batch)
 {
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };
+	struct msghdr msg;
+	memset(&msg, 0, sizeof(msg));
 	struct bio_vec bvec;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->rsp_pdu) - cmd->offset + hdgst;
 	int ret;
+	struct homa_sendmsg_args homa_send_args;
+	memset(&homa_send_args, 0, sizeof(homa_send_args));
+	homa_send_args.id  = cmd->response_state.response_id;
 
-	if (!last_in_batch && cmd->queue->send_list_len)
-		msg.msg_flags |= MSG_MORE;
-	else
-		msg.msg_flags |= MSG_EOR;
+	msg.msg_name = &(cmd->response_state.source_addr);
+	msg.msg_namelen = sizeof((cmd->response_state.source_addr));
+	msg.msg_control = &homa_send_args;
+	msg.msg_controllen = sizeof(homa_send_args);
+	msg.msg_control_is_user = false;
 
+	if(cmd->wbytes_done){
+	pr_info("We have written %u bytes of data\n", cmd->wbytes_done);
+		bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
+		bvecs[cmd->req.sg_cnt +1] = bvec;
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 2,
+		bvecs[0].bv_len + cmd->req.transfer_len + left);
+		ret = sock_sendmsg(cmd->queue->sock, &msg);
+		if (ret < 0)
+			return ret;
+	}
+	else{
 	bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
 	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
 	ret = sock_sendmsg(cmd->queue->sock, &msg);
-	if (ret <= 0)
+		if (ret < 0)
 		return ret;
-	cmd->offset += ret;
-	left -= ret;
 
-	if (left)
+	}
+	cmd->offset += left;
+	left -= left;
+
+	/*if (left)
 		return -EAGAIN;
+		*/
 
 	nvmet_tcp_free_cmd_buffers(cmd);
 	cmd->queue->snd_cmd = NULL;
@@ -1655,7 +1674,7 @@ static void nvmet_tcp_data_ready(struct sock *sk)
 	read_lock_bh(&sk->sk_callback_lock);
 	queue = sk->sk_user_data;
 	if (likely(queue))
-		queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
+		queue_work( nvmet_tcp_wq, &queue->io_work);
 	read_unlock_bh(&sk->sk_callback_lock);
 }
 
-- 
2.37.2


From 23d07eac5c8a5569fc627c8d2bc085558618c04d Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:52:39 +0000
Subject: [PATCH 12/26] Format Send Data to use HOMA

---
 drivers/nvme/target/tcp.c | 69 ++++++++++++++++++++++++---------------
 1 file changed, 42 insertions(+), 27 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 07f2099789f7..77cf93f35df3 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -688,54 +688,69 @@ static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
 	return 1;
 }
 
-static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
+static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs, bool last_in_batch)
 {
 	struct nvmet_tcp_queue *queue = cmd->queue;
 	int ret;
+	int num_sgl_entries = cmd->req.sg_cnt;
+	int n = 1;
+	int send_len = bvecs[0].bv_len + cmd->req.transfer_len;
+	struct msghdr msg;
+	pr_info("Total number of sgl entries: %d\n", num_sgl_entries);
+	pr_info("Total number of bytes to send: %d\n", send_len);
 
-	while (cmd->cur_sg) {
-		struct msghdr msg = {
-			.msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES,
-		};
+	while (cmd->cur_sg && n<=num_sgl_entries) {
 		struct page *page = sg_page(cmd->cur_sg);
 		struct bio_vec bvec;
-		u32 left = cmd->cur_sg->length - cmd->offset;
+		u32 left = cmd->cur_sg->length;
 
-		if ((!last_in_batch && cmd->queue->send_list_len) ||
+		/*if ((!last_in_batch && cmd->queue->send_list_len) ||
 		    cmd->wbytes_done + left < cmd->req.transfer_len ||
 		    queue->data_digest || !queue->nvme_sq.sqhd_disabled)
-			msg.msg_flags |= MSG_MORE;
+			msg.msg_flags |= MSG_MORE;*/
 
 		bvec_set_page(&bvec, page, left, cmd->offset);
-		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
-		ret = sock_sendmsg(cmd->queue->sock, &msg);
-		if (ret <= 0)
-			return ret;
-
-		cmd->offset += ret;
-		cmd->wbytes_done += ret;
+		bvecs[n] = bvec;
 
-		/* Done with sg?*/
-		if (cmd->offset == cmd->cur_sg->length) {
+		/* I don't think we need to care about cmd->offset
+		as it only comes into play with a sendmsg that doesn't
+		send all the data*/	
+		//cmd->offset += left;
+		cmd->wbytes_done += left;
 			cmd->cur_sg = sg_next(cmd->cur_sg);
-			cmd->offset = 0;
+		n++;
 		}
-	}
-
-	if (queue->data_digest) {
-		cmd->state = NVMET_TCP_SEND_DDGST;
-		cmd->offset = 0;
-	} else {
 		if (queue->nvme_sq.sqhd_disabled) {
+		cmd->queue->snd_cmd = NULL;
+		nvmet_tcp_put_cmd(cmd);
+		memset(&msg, 0, sizeof(msg));
+		struct homa_sendmsg_args homa_args;
+		memset(&homa_args, 0, sizeof(homa_args));
+		homa_args.id = cmd->response_state.response_id;
+		msg.msg_name = &(cmd->response_state.source_addr);
+		msg.msg_namelen = sizeof(cmd->response_state.source_addr);
+		msg.msg_control = &homa_args;
+		msg.msg_controllen = sizeof(homa_args);
+		msg.msg_control_is_user = false;
+
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, num_sgl_entries + 1, send_len);
+		ret = sock_sendmsg(cmd->queue->sock, &msg);
+		pr_info("Send data sock_sendmsg returned: %d\n", ret);
+		if (ret < 0){
+			return ret;
+		}
 			cmd->queue->snd_cmd = NULL;
 			nvmet_tcp_put_cmd(cmd);
+		nvmet_tcp_free_cmd_buffers(cmd);
 		} else {
+		pr_info("sqhd is :%d\n", queue->nvme_sq.sqhd_disabled);
 			nvmet_setup_response_pdu(cmd);
-		}
 	}
 
-	if (queue->nvme_sq.sqhd_disabled)
-		nvmet_tcp_free_cmd_buffers(cmd);
+	if (queue->data_digest) {
+		cmd->state = NVMET_TCP_SEND_DDGST;
+		cmd->offset = 0;
+	}
 
 	return 1;
 
-- 
2.37.2


From 1796a2d3626862aab22cf5f1efa4341e106960d5 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:53:29 +0000
Subject: [PATCH 13/26] Modify Send Data PDU to Work with HOMA

---
 drivers/nvme/target/tcp.c | 20 +++++++-------------
 1 file changed, 7 insertions(+), 13 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 77cf93f35df3..08985e230d02 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -661,30 +661,24 @@ static void nvmet_tcp_execute_request(struct nvmet_tcp_cmd *cmd)
 		cmd->req.execute(&cmd->req);
 }
 
-static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
+static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvec)
 {
-	struct msghdr msg = {
-		.msg_flags = MSG_DONTWAIT | MSG_MORE | MSG_SPLICE_PAGES,
-	};
-	struct bio_vec bvec;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
-	int ret;
-
-	bvec_set_virt(&bvec, (void *)cmd->data_pdu + cmd->offset, left);
-	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
-	ret = sock_sendmsg(cmd->queue->sock, &msg);
-	if (ret <= 0)
-		return ret;
 
+	bvec_set_virt(bvec, (void *)cmd->data_pdu + cmd->offset, left);
+	pr_info("Length of data pdu: %d\n", bvec->bv_len);
+	/*
+	do we even need this anymore?	
 	cmd->offset += ret;
 	left -= ret;
 
 	if (left)
 		return -EAGAIN;
-
+	*/
 	cmd->state = NVMET_TCP_SEND_DATA;
 	cmd->offset  = 0;
+	// do we even need this return?
 	return 1;
 }
 
-- 
2.37.2


From cddd552c135a210aab9e25b3cb390280d8aed55d Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 26 Oct 2023 14:54:42 +0000
Subject: [PATCH 14/26] Queue Work on Main CPU

---
 drivers/nvme/target/tcp.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 08985e230d02..7d11d657e6fa 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -650,7 +650,7 @@ static void nvmet_tcp_queue_response(struct nvmet_req *req)
 	}
 
 	llist_add(&cmd->lentry, &queue->resp_list);
-	queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &cmd->queue->io_work);
+	queue_work(nvmet_tcp_wq, &cmd->queue->io_work);
 }
 
 static void nvmet_tcp_execute_request(struct nvmet_tcp_cmd *cmd)
-- 
2.37.2


From b55ffc349b8965ee155e1d3a8793a258b306d953 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Mon, 13 Nov 2023 17:27:19 +0000
Subject: [PATCH 15/26] Make Changes to Enable Single Target Socket

---
 drivers/nvme/target/tcp.c | 293 +++++++++++++++++++++++++++-----------
 include/linux/nvme-tcp.h  |   8 +-
 2 files changed, 212 insertions(+), 89 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 7d11d657e6fa..9be75307e6c8 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -157,6 +157,8 @@ struct nvmet_tcp_queue {
 	int			offset;
 	int			left;
 	struct nvmet_homa_socket_response_state response_state;
+	struct homa_recvmsg_args *homa_recv_args;
+	bool ready_to_read_homa_pool;
 	enum nvmet_tcp_recv_state rcv_state;
 	struct nvmet_tcp_cmd	*cmd;
 	union nvme_tcp_pdu	pdu;
@@ -190,9 +192,12 @@ struct nvmet_tcp_queue {
 
 struct nvmet_tcp_port {
 	struct socket		*sock;
+	struct work_struct	read_sock;
 	struct homa_set_buf_args homa_buf_args;
+	struct homa_recvmsg_args homa_recv_args;
 	struct work_struct	accept_work;
 	struct nvmet_port	*nport;
+	struct nvmet_tcp_queue *queue_arr[2];
 	struct sockaddr_storage addr;
 	void (*data_ready)(struct sock *);
 };
@@ -223,8 +228,6 @@ static int copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, int len
 	struct nvmet_tcp_queue *queue, void *copy_to){
 
 	pr_info("HOMA response id : %llu\n", (unsigned long long) homa_recv_args->id);
-	pr_info("HOMA completion cookie : %llu\n", (unsigned long long)homa_recv_args->completion_cookie);
-	pr_info("The offset we are working with is %d: \n", *offset);
 
 	int data_offset = *offset & (HOMA_BPAGE_SIZE - 1);
 	int page_offset = (*offset / HOMA_BPAGE_SIZE);	
@@ -628,6 +631,7 @@ static struct nvmet_tcp_cmd *nvmet_tcp_fetch_cmd(struct nvmet_tcp_queue *queue)
 
 static void nvmet_tcp_queue_response(struct nvmet_req *req)
 {
+	pr_info("This function is being called\n");
 	struct nvmet_tcp_cmd *cmd =
 		container_of(req, struct nvmet_tcp_cmd, req);
 	struct nvmet_tcp_queue	*queue = cmd->queue;
@@ -650,6 +654,7 @@ static void nvmet_tcp_queue_response(struct nvmet_req *req)
 	}
 
 	llist_add(&cmd->lentry, &queue->resp_list);
+	//here we have to think why we need to queue this as deferred work
 	queue_work(nvmet_tcp_wq, &cmd->queue->io_work);
 }
 
@@ -715,8 +720,6 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		n++;
 		}
 		if (queue->nvme_sq.sqhd_disabled) {
-		cmd->queue->snd_cmd = NULL;
-		nvmet_tcp_put_cmd(cmd);
 		memset(&msg, 0, sizeof(msg));
 		struct homa_sendmsg_args homa_args;
 		memset(&homa_args, 0, sizeof(homa_args));
@@ -728,7 +731,7 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		msg.msg_control_is_user = false;
 
 		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, num_sgl_entries + 1, send_len);
-		ret = sock_sendmsg(cmd->queue->sock, &msg);
+		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
 		pr_info("Send data sock_sendmsg returned: %d\n", ret);
 		if (ret < 0){
 			return ret;
@@ -775,14 +778,14 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 		bvecs[cmd->req.sg_cnt +1] = bvec;
 		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 2,
 		bvecs[0].bv_len + cmd->req.transfer_len + left);
-		ret = sock_sendmsg(cmd->queue->sock, &msg);
+		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
 		if (ret < 0)
 			return ret;
 	}
 	else{
 	bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
 	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
-	ret = sock_sendmsg(cmd->queue->sock, &msg);
+		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
 		if (ret < 0)
 		return ret;
 
@@ -844,7 +847,7 @@ static int nvmet_try_send_ddgst(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 	else
 		msg.msg_flags |= MSG_EOR;
 
-	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	ret = kernel_sendmsg(queue->port->sock, &msg, &iov, 1, iov.iov_len);
 	if (unlikely(ret <= 0))
 		return ret;
 
@@ -871,8 +874,10 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 
 	if (!cmd || queue->state == NVMET_TCP_Q_DISCONNECTING) {
 		cmd = nvmet_tcp_fetch_cmd(queue);
-		if (unlikely(!cmd))
+		if (unlikely(!cmd)){
+			pr_info("No command to be found breaking out of the send\n");
 			return 0;
+		}
 	}
 	pr_info("Num of sg pages is : %d\n", cmd->req.sg_cnt);
 	struct bio_vec *data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
@@ -1041,7 +1046,7 @@ static int nvmet_tcp_handle_icreq(struct nvmet_tcp_queue *queue)
 	msg.msg_control_is_user = false;
 	iov.iov_base = icresp;
 	iov.iov_len = sizeof(*icresp);
-	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	ret = kernel_sendmsg(queue->port->sock, &msg, &iov, 1, iov.iov_len);
 	if (ret < 0)
 		goto free_crypto;
 
@@ -1112,6 +1117,8 @@ static int nvmet_tcp_handle_h2c_data_pdu(struct nvmet_tcp_queue *queue)
 		return -EPROTO;
 	}
 
+	cmd->response_state.source_addr = queue->response_state.source_addr;
+	cmd->response_state.response_id = queue->response_state.response_id;
 	cmd->pdu_len = le32_to_cpu(data->data_length);
 	cmd->pdu_recv = 0;
 	nvmet_tcp_build_pdu_iovec(cmd);
@@ -1154,8 +1161,12 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 	}
 
 	pr_info("SOme sanity checks!!!\n");
+	if(nvme_is_fabrics(nvme_cmd)){
+		pr_info("This is a fabric command\n");
 	pr_info("COmmand identifier is %d\n", nvme_cmd->fabrics.command_id);
 	pr_info("COmmnad fctype is %d\n", nvme_cmd->fabrics.fctype);
+	}
+	pr_info("COmmand identifier is %d\n", nvme_cmd->common.command_id);
 
 	queue->cmd = nvmet_tcp_get_cmd(queue);
 	if (unlikely(!queue->cmd)) {
@@ -1247,7 +1258,6 @@ struct homa_recvmsg_args *homa_recv_args, int *offset)
 	struct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;
 	int len = (int) sizeof(struct nvme_tcp_hdr);
 
-	pr_info("Copying common header!, num bits: %d\n", len);
 	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
 
 	queue->left -= len;
@@ -1267,14 +1277,10 @@ struct homa_recvmsg_args *homa_recv_args, int *offset)
 			pr_err("pdu %d bad hlen %d\n", hdr->type, hdr->hlen);
 			return -EIO;
 		}
-	pr_info("Header type: %d\n", hdr->type);
-	pr_info("Header len: %d\n", hdr->hlen);
-
 	/*queue->left = hdr->hlen - queue->offset + hdgst;
 	goto recv;*/
 
-	len = hdr->hlen - queue->offset;;
-	pr_info("Copying protocol specific header!, num bits: %d\n", len);
+	len = hdr->hlen - queue->offset; //VERIFY THiS DEFNITELY
 	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
 	queue->left -= len;
 
@@ -1370,32 +1376,8 @@ static int nvmet_tcp_try_recv_ddgst(struct nvmet_tcp_queue *queue)
 static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 {
 	int result = 0;
-	int len;
-	sockaddr_in_union source;
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
-	struct homa_recvmsg_args *homa_recv_args;
-
-	homa_recv_args = kzalloc(sizeof(*homa_recv_args), GFP_KERNEL);
-	homa_recv_args->flags |= HOMA_RECVMSG_REQUEST; 
-	homa_recv_args->flags |= HOMA_RECVMSG_NONBLOCKING;
-	homa_recv_args->num_bpages = 0;
-	msg.msg_control = homa_recv_args;
-	msg.msg_controllen = sizeof(*homa_recv_args);
-	msg.msg_name = &(source);
-	msg.msg_namelen = sizeof(source);
-	len = sock_recvmsg(queue->sock, &msg, msg.msg_flags);
-	if (len == -EAGAIN) {
-		pr_info("sock_recvmsg returned EAGAIN\n");
-		return 0;
-	}
-	if (unlikely(len < 0)){
-		pr_info("sock_recvmsg error returned %d\n", len);
-		return len;
-	}
-	pr_info("length of message received is : %d\n", len);
 
-	queue->response_state.response_id = homa_recv_args->id;
-	memcpy(&queue->response_state.source_addr, msg.msg_name, sizeof(sockaddr_in_union));
+	struct homa_recvmsg_args *homa_recv_args = queue->homa_recv_args;
 
 	if (unlikely(queue->rcv_state == NVMET_TCP_RECV_ERR))
 		return 0;
@@ -1425,12 +1407,13 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 	}
 
 done_recv:
-	kfree(homa_recv_args);
+	queue->ready_to_read_homa_pool = false;
 	if (result < 0) {
 		if (result == -EAGAIN)
 			return 0;
 		return result;
 	}
+	pr_info("Done with function\n");
 	return 1;
 }
 
@@ -1438,19 +1421,34 @@ static int nvmet_tcp_try_recv(struct nvmet_tcp_queue *queue,
 		int budget, int *recvs)
 {
 	int i, ret = 0;
-
-	for (i = 0; i < budget; i++) {
-		ret = nvmet_tcp_try_recv_one(queue);
-		if (unlikely(ret < 0)) {
-			nvmet_tcp_socket_error(queue, ret);
-			goto done;
-		} else if (ret == 0) {
-			break;
-		}
-		(*recvs)++;
-	}
-done:
-	return ret;
+	/*Some thoughts on this section:
+		- lol copilot don't know shit
+		- in the tcp model:
+		io-work------>---------
+		^		      |
+		|		      |
+		|		      |
+		|		      v
+	  |---->-------|	|----->-------|
+	  ^  send work V	^  recv work  v
+	  |--------<---|	|-----<-------|
+		|			|
+		^			v
+		|-----------<----------	|
+
+		the outer loop works for tcp as we can fit more than 1 pdu in a 
+		packet, so we may want to read repeatedly. The inner loop works 
+		because we repeateadly rcv_msg a part of the message in the socket.
+		So it would only loop for failures. In HOMA we don't tolerate failure.
+		Lol jk, but any failures in our recv_msg should have been handled much
+		much earlier. So here we can concentrate on just receiving the packet.
+		This led me to think about refactoring, which led me to write this. Potentially
+		the outer loop is uncecessary as well. As we are maintaining Homas RPC model,
+		I don't see if for any packets received we will need do a recv, send loop more than
+		once. Ofc we can potentially implement many pdu in an RPC but I don't thik that is 
+		optimal. So I will leave the outer loop until I am sure of that decision.
+	*/
+	return nvmet_tcp_try_recv_one(queue);
 }
 
 static void nvmet_tcp_schedule_release_queue(struct nvmet_tcp_queue *queue)
@@ -1480,35 +1478,98 @@ static bool nvmet_tcp_check_queue_deadline(struct nvmet_tcp_queue *queue,
 	return !time_after(jiffies, queue->poll_end);
 }
 
+/*static void nvmet_homa_read_sock(struct work_struct *w){
+	struct nvmet_tcp_queue *queue =
+		container_of(w, struct nvmet_tcp_queue, io_work);
+	bool pending;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct homa_recvmsg_args *homa_recv_args;
+	sockaddr_in_union source;
+	int ret, ops, len = 0;
+	homa_recv_args = kzalloc(sizeof(*homa_recv_args), GFP_KERNEL);
+	homa_recv_args->flags |= HOMA_RECVMSG_REQUEST; 
+	homa_recv_args->flags |= HOMA_RECVMSG_NONBLOCKING;
+	homa_recv_args->num_bpages = 0;
+	msg.msg_control = homa_recv_args;
+	msg.msg_controllen = sizeof(*homa_recv_args);
+	msg.msg_name = &(source);
+	msg.msg_namelen = sizeof(source);
+	len = sock_recvmsg(queue->sock, &msg, msg.msg_flags);
+	if (len == -EAGAIN) {
+		pr_info("sock_recvmsg returned EAGAIN\n");
+		return 0;
+	}
+	if (unlikely(len < 0)){
+		pr_info("sock_recvmsg error returned %d\n", len);
+		return len;
+	}
+	pr_info("length of message received is : %d\n", len);
+
+	//read what the queue id is and pass it on
+	int len = (int) sizeof(struct nvme_tcp_hdr);
+
+	pr_info("Copying common header!, num bits: %d\n", len);
+	copy_from_homa_pool(homa_recv_args, len , 0, queue, &queue->pdu);
+
+	if(queue->pdu.cmd.hdr->queue_id){
+		//queue rest of the work on the respective core
+		return;
+	}
+
+
+	//call homa_read_pool then?
+
+}*/
+
+static void nvmet_homa_io_work(struct nvmet_tcp_queue *queue){
+	int ret, ops = 0;
+	pr_info("Entering io_work function\n");
+
+
+	if(queue->ready_to_read_homa_pool){
+		ret = nvmet_tcp_try_recv(queue, NVMET_TCP_RECV_BUDGET, &ops);
+		if (ret < 0)
+			return;
+	}
+
+	ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
+	if (ret < 0)
+		return;
+
+
+	/*
+	 * Requeue the worker if idle deadline period is in progress or any
+	 * ops activity was recorded during the do-while loop above.
+	 */
+	if (nvmet_tcp_check_queue_deadline(queue, ops))
+		queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
+
+}
+
 static void nvmet_tcp_io_work(struct work_struct *w)
 {
 	struct nvmet_tcp_queue *queue =
 		container_of(w, struct nvmet_tcp_queue, io_work);
-	bool pending;
 	int ret, ops = 0;
+	pr_info("Entering io_work queue \n");
 
-	do {
-		pending = false;
 
+	if(queue->ready_to_read_homa_pool){
 		ret = nvmet_tcp_try_recv(queue, NVMET_TCP_RECV_BUDGET, &ops);
-		if (ret > 0)
-			pending = true;
-		else if (ret < 0)
+		if (ret < 0)
 			return;
+	}
 
 		ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
-		if (ret > 0)
-			pending = true;
-		else if (ret < 0)
+	if (ret < 0)
 			return;
 
-	} while (pending && ops < NVMET_TCP_IO_WORK_BUDGET);
 
 	/*
 	 * Requeue the worker if idle deadline period is in progress or any
 	 * ops activity was recorded during the do-while loop above.
 	 */
-	if (nvmet_tcp_check_queue_deadline(queue, ops) || pending)
+	if (nvmet_tcp_check_queue_deadline(queue, ops))
 		queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
 }
 
@@ -1674,21 +1735,27 @@ static void nvmet_tcp_release_queue_work(struct work_struct *w)
 	kfree(queue);
 }
 
+
 static void nvmet_tcp_data_ready(struct sock *sk)
 {
-	struct nvmet_tcp_queue *queue;
+	read_lock_bh(&sk->sk_callback_lock);
+	struct nvmet_tcp_port *port = sk->sk_user_data;
+	read_unlock_bh(&sk->sk_callback_lock);
 
 	trace_sk_data_ready(sk);
+	queue_work( nvmet_tcp_wq, &port->read_sock);
 
-	read_lock_bh(&sk->sk_callback_lock);
-	queue = sk->sk_user_data;
-	if (likely(queue))
-		queue_work( nvmet_tcp_wq, &queue->io_work);
-	read_unlock_bh(&sk->sk_callback_lock);
+
+	/*
+	when dis get called we quickly read the first few bytes of the message for
+	the queue id and then pass it on to the respective queue? 
+	*/
 }
 
 static void nvmet_tcp_write_space(struct sock *sk)
 {
+	//I don't think we need this function cause it seems to have more
+	//to do with filling up TCP buffers. Should read up on it.
 	struct nvmet_tcp_queue *queue;
 
 	read_lock_bh(&sk->sk_callback_lock);
@@ -1739,7 +1806,6 @@ static void nvmet_tcp_state_change(struct sock *sk)
 static int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)
 {
 	struct socket *sock = queue->sock;
-	struct inet_sock *inet = inet_sk(sock->sk);
 	int ret;
 	//TODO: COME HERE AND CLEAR UP LARGE COMMENTED OUT BLOCKS
 	/*ret = kernel_getsockname(sock,
@@ -1775,27 +1841,25 @@ static int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)
 	sock->sk->sk_state_change = nvmet_tcp_state_change;
 	queue->write_space = sock->sk->sk_write_space;
 	sock->sk->sk_write_space = nvmet_tcp_write_space;
-	if (idle_poll_period_usecs)
-		nvmet_tcp_arm_queue_deadline(queue);
 	//queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
 	write_unlock_bh(&sock->sk->sk_callback_lock);
 
 	return ret;
 }
 
-static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
-		struct socket *newsock)
+static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port, int queue_id)
 {
 	struct nvmet_tcp_queue *queue;
 	int ret;
+	
 
 	queue = kzalloc(sizeof(*queue), GFP_KERNEL);
 	if (!queue)
 		return -ENOMEM;
+	port->queue_arr[queue_id] = queue;
 
 	INIT_WORK(&queue->release_work, nvmet_tcp_release_queue_work);
 	INIT_WORK(&queue->io_work, nvmet_tcp_io_work);
-	queue->sock = newsock;
 	queue->port = port;
 	queue->nr_cmds = 0;
 	spin_lock_init(&queue->state_lock);
@@ -1824,9 +1888,8 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,
 	list_add_tail(&queue->queue_list, &nvmet_tcp_queue_list);
 	mutex_unlock(&nvmet_tcp_queue_mutex);
 
-	ret = nvmet_tcp_set_queue_sock(queue);
-	if (ret)
-		goto out_destroy_sq;
+	if (idle_poll_period_usecs)
+		nvmet_tcp_arm_queue_deadline(queue);
 
 	return 0;
 out_destroy_sq:
@@ -1857,13 +1920,65 @@ static void nvmet_tcp_accept_work(struct work_struct *w)
 				pr_warn("failed to accept err=%d\n", ret);
 			return;
 		}
-		ret = nvmet_tcp_alloc_queue(port, newsock);
+		//ret = nvmet_tcp_alloc_queue(port, newsock);
 		if (ret) {
 			pr_err("failed to allocate queue\n");
 			sock_release(newsock);
 		}
 	}
 }
+static void nvmet_homa_read_sock(struct work_struct *w){
+	//This function can't be prempted by another read_sock
+	//As they it would only be queued after this one is done.
+	struct nvmet_tcp_queue *queue;
+	struct nvmet_tcp_port *port = 
+		container_of(w, struct nvmet_tcp_port, read_sock);
+	int len;
+	sockaddr_in_union source;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct homa_recvmsg_args *homa_recv_args = &port->homa_recv_args;
+
+	union nvme_tcp_pdu pdu;  
+	homa_recv_args->flags |= HOMA_RECVMSG_REQUEST; 
+	homa_recv_args->flags |= HOMA_RECVMSG_NONBLOCKING;
+	homa_recv_args->id = 0;
+	msg.msg_control = homa_recv_args;
+	msg.msg_controllen = sizeof(*homa_recv_args);
+	msg.msg_name = &(source);
+	msg.msg_namelen = sizeof(source);
+	len = sock_recvmsg(port->sock, &msg, msg.msg_flags);
+	if (unlikely(len < 0)){
+		pr_info("sock_recvmsg error returned %d\n", len);
+		//nvmet_tcp_socket_error(queue, len);
+		goto done;
+	}
+	pr_info("length of message received : %d\n", len);
+	len = (int) sizeof(struct nvme_tcp_hdr);
+
+	memcpy(&pdu,port->homa_buf_args.start + homa_recv_args->bpage_offsets[0], len);
+	pr_info("Header type: %d\n", pdu.cmd.hdr.type);
+	pr_info("Header len: %d\n", pdu.cmd.hdr.hlen);
+	pr_info("Origin queue: %d\n", pdu.cmd.hdr.queue_id);
+
+	if (port->queue_arr[pdu.cmd.hdr.queue_id] == NULL){
+		pr_info("Queue is null\n");
+		nvmet_tcp_alloc_queue(port, pdu.cmd.hdr.queue_id);
+	}
+	queue = port->queue_arr[pdu.cmd.hdr.queue_id];
+	queue->response_state.response_id = homa_recv_args->id;
+	//Leaving it in for now, but if we have recv_msg_args in
+	//the port do we need to have this here cause it will last
+	//as long as we care I was thinking. But good for readability.
+	queue->homa_recv_args = homa_recv_args;
+	memcpy(&queue->response_state.source_addr, msg.msg_name, sizeof(sockaddr_in_union));
+	queue->ready_to_read_homa_pool = true;
+	nvmet_homa_io_work(queue);
+	return;
+
+done:
+	return;
+
+}
 
 static void nvmet_tcp_listen_data_ready(struct sock *sk)
 {
@@ -1918,6 +2033,7 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 
 	port->nport = nport;
 	//INIT_WORK(&port->accept_work, nvmet_tcp_accept_work);
+	INIT_WORK(&port->read_sock, nvmet_homa_read_sock);
 	if (port->nport->inline_data_size < 0)
 		port->nport->inline_data_size = NVMET_TCP_DEF_INLINE_DATA_SIZE;
 
@@ -1928,9 +2044,14 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 		goto err_port;
 	}
 
+	write_lock_bh(&port->sock->sk->sk_callback_lock);
+	port->sock->sk->sk_user_data = port;
+	port->sock->sk->sk_data_ready = nvmet_tcp_data_ready;
+	port->sock->sk->sk_state_change = nvmet_tcp_state_change;
+	port->sock->sk->sk_write_space = nvmet_tcp_write_space;
+	write_unlock_bh(&port->sock->sk->sk_callback_lock);
 	port->homa_buf_args.length = bufsize;
 	port->homa_buf_args.start = server_homa_buf_region;
-	port->sock->sk->sk_user_data = port;
 	port->data_ready = port->sock->sk->sk_data_ready;
 	ret = port->sock->ops->setsockopt(
 		port->sock, IPPROTO_HOMA, SO_HOMA_SET_BUF,KERNEL_SOCKPTR(&(port->homa_buf_args)),
@@ -1953,12 +2074,12 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 	pr_info("enabling port %d (%pISpc)\n",
 		le16_to_cpu(nport->disc_addr.portid), &port->addr);
 	
-	ret = nvmet_tcp_alloc_queue(port, port->sock);
+	/*ret = nvmet_tcp_alloc_queue(port, port->sock);
 	if (ret){
 		pr_err("Failed to allocate queue\n");
 		sock_release(port->sock);
 		kfree(server_homa_buf_region);
-	}
+	}*/
 
 	return 0;
 
@@ -1991,6 +2112,7 @@ static void nvmet_tcp_remove_port(struct nvmet_port *nport)
 	port->sock->sk->sk_user_data = NULL;
 	write_unlock_bh(&port->sock->sk->sk_callback_lock);
 	cancel_work_sync(&port->accept_work);
+	cancel_work_sync(&port->read_sock);
 	/*
 	 * Destroy the remaining queues, which are not belong to any
 	 * controller yet.
@@ -2009,7 +2131,6 @@ static void nvmet_tcp_delete_ctrl(struct nvmet_ctrl *ctrl)
 	mutex_lock(&nvmet_tcp_queue_mutex);
 	list_for_each_entry(queue, &nvmet_tcp_queue_list, queue_list)
 		if (queue->nvme_sq.ctrl == ctrl)
-			kernel_sock_shutdown(queue->sock, SHUT_RDWR);
 	mutex_unlock(&nvmet_tcp_queue_mutex);
 }
 
diff --git a/include/linux/nvme-tcp.h b/include/linux/nvme-tcp.h
index 57ebe1267f7f..f577c09f3123 100644
--- a/include/linux/nvme-tcp.h
+++ b/include/linux/nvme-tcp.h
@@ -55,6 +55,7 @@ enum nvme_tcp_pdu_flags {
 /**
  * struct nvme_tcp_hdr - nvme tcp pdu common header
  *
+ * @queue_id:	   pdu_origin_queue_id
  * @type:          pdu type
  * @flags:         pdu specific flags
  * @hlen:          pdu header length
@@ -62,6 +63,7 @@ enum nvme_tcp_pdu_flags {
  * @plen:          pdu wire byte length
  */
 struct nvme_tcp_hdr {
+	__u8	queue_id;
 	__u8	type;
 	__u8	flags;
 	__u8	hlen;
@@ -117,7 +119,7 @@ struct nvme_tcp_term_pdu {
 	__le16			fes;
 	__le16			feil;
 	__le16			feiu;
-	__u8			rsvd[10];
+	__u8			rsvd[14];
 };
 
 /**
@@ -158,7 +160,7 @@ struct nvme_tcp_r2t_pdu {
 	__u16			ttag;
 	__le32			r2t_offset;
 	__le32			r2t_length;
-	__u8			rsvd[4];
+	__u8			rsvd[8];
 };
 
 /**
@@ -176,7 +178,7 @@ struct nvme_tcp_data_pdu {
 	__u16			ttag;
 	__le32			data_offset;
 	__le32			data_length;
-	__u8			rsvd[4];
+	__u8			rsvd[8];
 };
 
 union nvme_tcp_pdu {
-- 
2.37.2


From 2cfc974132013f0f86b96463294bb7cb27a4c112 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Mon, 13 Nov 2023 17:43:39 +0000
Subject: [PATCH 16/26] Enable R2T

---
 drivers/nvme/target/tcp.c | 80 +++++++++++++++++++++++----------------
 1 file changed, 48 insertions(+), 32 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 9be75307e6c8..ccfbb530bfeb 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -137,6 +137,7 @@ enum nvmet_tcp_queue_state {
 	NVMET_TCP_Q_DISCONNECTING,
 };
 
+
 struct nvmet_tcp_queue {
 	struct socket		*sock;
 	struct nvmet_tcp_port	*port;
@@ -548,6 +549,7 @@ static void nvmet_setup_c2h_data_pdu(struct nvmet_tcp_cmd *cmd)
 
 static void nvmet_setup_r2t_pdu(struct nvmet_tcp_cmd *cmd)
 {
+	pr_info("Setting up RT2\n");
 	struct nvme_tcp_r2t_pdu *pdu = cmd->r2t_pdu;
 	struct nvmet_tcp_queue *queue = cmd->queue;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
@@ -675,7 +677,7 @@ static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 	pr_info("Length of data pdu: %d\n", bvec->bv_len);
 	/*
 	do we even need this anymore?	
-	cmd->offset += ret;
+		cmd->offset += ret;
 	left -= ret;
 
 	if (left)
@@ -716,10 +718,10 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		send all the data*/	
 		//cmd->offset += left;
 		cmd->wbytes_done += left;
-			cmd->cur_sg = sg_next(cmd->cur_sg);
+		cmd->cur_sg = sg_next(cmd->cur_sg);
 		n++;
-		}
-		if (queue->nvme_sq.sqhd_disabled) {
+	}
+	if (queue->nvme_sq.sqhd_disabled) {
 		memset(&msg, 0, sizeof(msg));
 		struct homa_sendmsg_args homa_args;
 		memset(&homa_args, 0, sizeof(homa_args));
@@ -736,12 +738,12 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		if (ret < 0){
 			return ret;
 		}
-			cmd->queue->snd_cmd = NULL;
-			nvmet_tcp_put_cmd(cmd);
+		cmd->queue->snd_cmd = NULL;
+		nvmet_tcp_put_cmd(cmd);
 		nvmet_tcp_free_cmd_buffers(cmd);
-		} else {
+	} else {
 		pr_info("sqhd is :%d\n", queue->nvme_sq.sqhd_disabled);
-			nvmet_setup_response_pdu(cmd);
+		nvmet_setup_response_pdu(cmd);
 	}
 
 	if (queue->data_digest) {
@@ -783,11 +785,11 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 			return ret;
 	}
 	else{
-	bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
-	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+		bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
 		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
 		if (ret < 0)
-		return ret;
+			return ret;
 
 	}
 	cmd->offset += left;
@@ -805,28 +807,41 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 
 static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 {
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };
+	pr_info("We are here!");
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
 	struct bio_vec bvec;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->r2t_pdu) - cmd->offset + hdgst;
 	int ret;
 
+	/*
 	if (!last_in_batch && cmd->queue->send_list_len)
 		msg.msg_flags |= MSG_MORE;
 	else
 		msg.msg_flags |= MSG_EOR;
+	*/
 
 	bvec_set_virt(&bvec, (void *)cmd->r2t_pdu + cmd->offset, left);
 	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
-	ret = sock_sendmsg(cmd->queue->sock, &msg);
-	if (ret <= 0)
+	struct homa_sendmsg_args homa_send_args;
+	memset(&homa_send_args, 0, sizeof(homa_send_args));
+	homa_send_args.id  = cmd->response_state.response_id;
+
+	msg.msg_name = &(cmd->response_state.source_addr);
+	msg.msg_namelen = sizeof((cmd->response_state.source_addr));
+	msg.msg_control = &homa_send_args;
+	msg.msg_controllen = sizeof(homa_send_args);
+	msg.msg_control_is_user = false;
+	ret = sock_sendmsg(cmd->queue->port->sock, &msg);
+	if (ret < 0)
 		return ret;
-	cmd->offset += ret;
-	left -= ret;
+	cmd->offset += left;
+	left -= left;
 
+ /*
 	if (left)
 		return -EAGAIN;
-
+	*/
 	cmd->queue->snd_cmd = NULL;
 	return 1;
 }
@@ -1163,8 +1178,8 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 	pr_info("SOme sanity checks!!!\n");
 	if(nvme_is_fabrics(nvme_cmd)){
 		pr_info("This is a fabric command\n");
-	pr_info("COmmand identifier is %d\n", nvme_cmd->fabrics.command_id);
-	pr_info("COmmnad fctype is %d\n", nvme_cmd->fabrics.fctype);
+		pr_info("COmmand identifier is %d\n", nvme_cmd->fabrics.command_id);
+		pr_info("COmmnad fctype is %d\n", nvme_cmd->fabrics.fctype);
 	}
 	pr_info("COmmand identifier is %d\n", nvme_cmd->common.command_id);
 
@@ -1265,21 +1280,21 @@ struct homa_recvmsg_args *homa_recv_args, int *offset)
 
 
 	//if (queue->offset == sizeof(struct nvme_tcp_hdr)) 
-		u8 hdgst = nvmet_tcp_hdgst_len(queue);
+	u8 hdgst = nvmet_tcp_hdgst_len(queue);
 
-		if (unlikely(!nvmet_tcp_pdu_valid(hdr->type))) {
-			pr_err("unexpected pdu type %d\n", hdr->type);
-			nvmet_tcp_fatal_error(queue);
-			return -EIO;
-		}
+	if (unlikely(!nvmet_tcp_pdu_valid(hdr->type))) {
+		pr_err("unexpected pdu type %d\n", hdr->type);
+		nvmet_tcp_fatal_error(queue);
+		return -EIO;
+	}
 
-		if (unlikely(hdr->hlen != nvmet_tcp_pdu_size(hdr->type))) {
-			pr_err("pdu %d bad hlen %d\n", hdr->type, hdr->hlen);
-			return -EIO;
-		}
+	if (unlikely(hdr->hlen != nvmet_tcp_pdu_size(hdr->type))) {
+		pr_err("pdu %d bad hlen %d\n", hdr->type, hdr->hlen);
+		return -EIO;
+	}
 	/*queue->left = hdr->hlen - queue->offset + hdgst;
 	goto recv;*/
-
+	
 	len = hdr->hlen - queue->offset; //VERIFY THiS DEFNITELY
 	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
 	queue->left -= len;
@@ -1560,9 +1575,9 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 			return;
 	}
 
-		ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
+	ret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);
 	if (ret < 0)
-			return;
+		return;
 
 
 	/*
@@ -2004,6 +2019,7 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 	int bufsize = 64 * HOMA_BPAGE_SIZE;
 	int ret;
 
+
 	port = kzalloc(sizeof(*port), GFP_KERNEL);
 	if (!port)
 		return -ENOMEM;
-- 
2.37.2


From 4c8b9fe53267b83722c7dab31d14fee300d9a86e Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Mon, 13 Nov 2023 18:26:58 +0000
Subject: [PATCH 17/26] Change Most of the Debug Message to pr_debug

---
 drivers/nvme/target/tcp.c | 69 ++++++++++++++++++---------------------
 1 file changed, 31 insertions(+), 38 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index ccfbb530bfeb..47ded8145f03 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -228,7 +228,7 @@ static void nvmet_tcp_free_cmd_buffers(struct nvmet_tcp_cmd *cmd);
 static int copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, int len_to_copy, unsigned int *offset,
 	struct nvmet_tcp_queue *queue, void *copy_to){
 
-	pr_info("HOMA response id : %llu\n", (unsigned long long) homa_recv_args->id);
+	pr_debug("HOMA response id : %llu\n", (unsigned long long) homa_recv_args->id);
 
 	int data_offset = *offset & (HOMA_BPAGE_SIZE - 1);
 	int page_offset = (*offset / HOMA_BPAGE_SIZE);	
@@ -549,7 +549,7 @@ static void nvmet_setup_c2h_data_pdu(struct nvmet_tcp_cmd *cmd)
 
 static void nvmet_setup_r2t_pdu(struct nvmet_tcp_cmd *cmd)
 {
-	pr_info("Setting up RT2\n");
+	pr_debug("Setting up RT2\n");
 	struct nvme_tcp_r2t_pdu *pdu = cmd->r2t_pdu;
 	struct nvmet_tcp_queue *queue = cmd->queue;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
@@ -633,7 +633,7 @@ static struct nvmet_tcp_cmd *nvmet_tcp_fetch_cmd(struct nvmet_tcp_queue *queue)
 
 static void nvmet_tcp_queue_response(struct nvmet_req *req)
 {
-	pr_info("This function is being called\n");
+	pr_debug("Queue response is being called\n");
 	struct nvmet_tcp_cmd *cmd =
 		container_of(req, struct nvmet_tcp_cmd, req);
 	struct nvmet_tcp_queue	*queue = cmd->queue;
@@ -674,7 +674,7 @@ static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
 
 	bvec_set_virt(bvec, (void *)cmd->data_pdu + cmd->offset, left);
-	pr_info("Length of data pdu: %d\n", bvec->bv_len);
+	pr_debug("Length of data pdu: %d\n", bvec->bv_len);
 	/*
 	do we even need this anymore?	
 		cmd->offset += ret;
@@ -697,8 +697,8 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 	int n = 1;
 	int send_len = bvecs[0].bv_len + cmd->req.transfer_len;
 	struct msghdr msg;
-	pr_info("Total number of sgl entries: %d\n", num_sgl_entries);
-	pr_info("Total number of bytes to send: %d\n", send_len);
+	pr_debug("Total number of sgl entries: %d\n", num_sgl_entries);
+	pr_debug("Total number of bytes to send: %d\n", send_len);
 
 	while (cmd->cur_sg && n<=num_sgl_entries) {
 		struct page *page = sg_page(cmd->cur_sg);
@@ -734,7 +734,7 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 
 		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, num_sgl_entries + 1, send_len);
 		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
-		pr_info("Send data sock_sendmsg returned: %d\n", ret);
+		pr_debug("Send data sock_sendmsg returned: %d\n", ret);
 		if (ret < 0){
 			return ret;
 		}
@@ -742,7 +742,6 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		nvmet_tcp_put_cmd(cmd);
 		nvmet_tcp_free_cmd_buffers(cmd);
 	} else {
-		pr_info("sqhd is :%d\n", queue->nvme_sq.sqhd_disabled);
 		nvmet_setup_response_pdu(cmd);
 	}
 
@@ -775,7 +774,6 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 	msg.msg_control_is_user = false;
 
 	if(cmd->wbytes_done){
-	pr_info("We have written %u bytes of data\n", cmd->wbytes_done);
 		bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
 		bvecs[cmd->req.sg_cnt +1] = bvec;
 		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 2,
@@ -807,7 +805,7 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 
 static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 {
-	pr_info("We are here!");
+	pr_debug("We in send_r2t!");
 	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
 	struct bio_vec bvec;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
@@ -890,30 +888,30 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 	if (!cmd || queue->state == NVMET_TCP_Q_DISCONNECTING) {
 		cmd = nvmet_tcp_fetch_cmd(queue);
 		if (unlikely(!cmd)){
-			pr_info("No command to be found breaking out of the send\n");
+			pr_debug("No command to be found breaking out of the send\n");
 			return 0;
 		}
 	}
-	pr_info("Num of sg pages is : %d\n", cmd->req.sg_cnt);
+	pr_debug("Num of sg pages is : %d\n", cmd->req.sg_cnt);
 	struct bio_vec *data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
 	, GFP_KERNEL);
 
 	if (cmd->state == NVMET_TCP_SEND_DATA_PDU) {
-		pr_info("Sending data pdu \n");
+		pr_debug("Sending data pdu \n");
 		ret = nvmet_try_send_data_pdu(cmd, data_bvecs);
 		if (ret <= 0)
 			goto done_send;
 	}
 
 	if (cmd->state == NVMET_TCP_SEND_DATA) {
-		pr_info("Sending data \n");
+		pr_debug("Sending data \n");
 		ret = nvmet_try_send_data(cmd, data_bvecs,last_in_batch);
 		if (ret <= 0)
 			goto done_send;
 	}
 
 	if (cmd->state == NVMET_TCP_SEND_DDGST) {
-		pr_info("Sending ddgst \n");
+		pr_debug("Sending ddgst \n");
 		ret = nvmet_try_send_ddgst(cmd, last_in_batch);
 		if (ret <= 0)
 			goto done_send;
@@ -926,7 +924,7 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 	}
 
 	if (cmd->state == NVMET_TCP_SEND_RESPONSE){
-		pr_info("Sending response \n");
+		pr_debug("Sending response \n");
 		ret = nvmet_try_send_response(cmd, data_bvecs,last_in_batch);
 	}
 
@@ -1157,7 +1155,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 			nvmet_tcp_fatal_error(queue);
 			return -EPROTO;
 		}
-		pr_info("Handling ICreq");
+		pr_debug("Handling ICreq");
 		return nvmet_tcp_handle_icreq(queue);
 	}
 
@@ -1175,13 +1173,10 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 		return 0;
 	}
 
-	pr_info("SOme sanity checks!!!\n");
 	if(nvme_is_fabrics(nvme_cmd)){
-		pr_info("This is a fabric command\n");
-		pr_info("COmmand identifier is %d\n", nvme_cmd->fabrics.command_id);
-		pr_info("COmmnad fctype is %d\n", nvme_cmd->fabrics.fctype);
+		pr_debug("Command identifier is %d\n", nvme_cmd->fabrics.command_id);
 	}
-	pr_info("COmmand identifier is %d\n", nvme_cmd->common.command_id);
+	pr_debug("Command identifier is %d\n", nvme_cmd->common.command_id);
 
 	queue->cmd = nvmet_tcp_get_cmd(queue);
 	if (unlikely(!queue->cmd)) {
@@ -1195,7 +1190,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 
 	req = &queue->cmd->req;
 	memcpy(req->cmd, nvme_cmd, sizeof(*nvme_cmd));
-	pr_info("succesfully copied nvme_cmd\n");
+	pr_debug("succesfully copied nvme_cmd\n");
 
 	if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
 			&queue->nvme_sq, &nvmet_tcp_ops))) {
@@ -1208,9 +1203,9 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 		return 0;
 	}
 
-	pr_info("beginning to map nvme_cmd\n");
+	pr_debug("beginning to map nvme_cmd\n");
 	ret = nvmet_tcp_map_data(queue->cmd);
-	pr_info("succesfully mapped nvme_cmd\n");
+	pr_debug("succesfully mapped nvme_cmd\n");
 	if (unlikely(ret)) {
 		pr_err("queue %d: failed to map data\n", queue->idx);
 		if (nvmet_tcp_has_inline_data(queue->cmd))
@@ -1233,7 +1228,7 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 	}
 
 	queue->cmd->req.execute(&queue->cmd->req);
-	pr_info("succhesfully executed nvme command\n");
+	pr_debug("succesfully executed nvme command\n");
 out:
 	nvmet_prepare_receive_pdu(queue);
 	return ret;
@@ -1328,7 +1323,7 @@ static int nvmet_tcp_try_recv_data(struct homa_recvmsg_args *homa_recv_args, str
 {
 	struct nvmet_tcp_cmd  *cmd = queue->cmd;
 
-	pr_info("Receiving data in this recvmsg\n");
+	pr_debug("Receiving data in this recvmsg\n");
 	int len = cmd->recv_msg.msg_iter.count;
 	/*Can probably remove the msghdr ting we got goin on*/
 	iter_copy_from_homa_pool(homa_recv_args, len, offset, queue, &cmd->recv_msg.msg_iter);
@@ -1398,7 +1393,7 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 		return 0;
 
 	if (queue->rcv_state == NVMET_TCP_RECV_PDU) {
-		pr_info("Trying to received pdu\n");
+		pr_debug("Trying to received pdu\n");
 		/*we can use queue->offset as our offset tracker into the homa ppol*/
 		/*^ Addendum to above comment this does not work lol, cause copy_to location
 		is queue->pdu + queue_offset so we would kinda be fokken up here*/
@@ -1409,7 +1404,7 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 	}
 
 	if (queue->rcv_state == NVMET_TCP_RECV_DATA) {
-		pr_info("Trying to received data\n");
+		pr_debug("Trying to received data\n");
 		result = nvmet_tcp_try_recv_data(homa_recv_args, queue, &queue->offset);
 		if (result != 0)
 			goto done_recv;
@@ -1428,7 +1423,6 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 			return 0;
 		return result;
 	}
-	pr_info("Done with function\n");
 	return 1;
 }
 
@@ -1538,7 +1532,7 @@ static bool nvmet_tcp_check_queue_deadline(struct nvmet_tcp_queue *queue,
 
 static void nvmet_homa_io_work(struct nvmet_tcp_queue *queue){
 	int ret, ops = 0;
-	pr_info("Entering io_work function\n");
+	pr_debug("Entering io_work function\n");
 
 
 	if(queue->ready_to_read_homa_pool){
@@ -1566,7 +1560,7 @@ static void nvmet_tcp_io_work(struct work_struct *w)
 	struct nvmet_tcp_queue *queue =
 		container_of(w, struct nvmet_tcp_queue, io_work);
 	int ret, ops = 0;
-	pr_info("Entering io_work queue \n");
+	pr_debug("Entering io_work queue \n");
 
 
 	if(queue->ready_to_read_homa_pool){
@@ -1963,20 +1957,19 @@ static void nvmet_homa_read_sock(struct work_struct *w){
 	msg.msg_namelen = sizeof(source);
 	len = sock_recvmsg(port->sock, &msg, msg.msg_flags);
 	if (unlikely(len < 0)){
-		pr_info("sock_recvmsg error returned %d\n", len);
+		pr_debug("sock_recvmsg error returned %d\n", len);
 		//nvmet_tcp_socket_error(queue, len);
 		goto done;
 	}
-	pr_info("length of message received : %d\n", len);
+	pr_debug("length of message received : %d\n", len);
 	len = (int) sizeof(struct nvme_tcp_hdr);
 
 	memcpy(&pdu,port->homa_buf_args.start + homa_recv_args->bpage_offsets[0], len);
-	pr_info("Header type: %d\n", pdu.cmd.hdr.type);
-	pr_info("Header len: %d\n", pdu.cmd.hdr.hlen);
-	pr_info("Origin queue: %d\n", pdu.cmd.hdr.queue_id);
+	pr_debug("Header type: %d\n", pdu.cmd.hdr.type);
+	pr_debug("Origin queue: %d\n", pdu.cmd.hdr.queue_id);
 
 	if (port->queue_arr[pdu.cmd.hdr.queue_id] == NULL){
-		pr_info("Queue is null\n");
+		pr_debug("Queue is null\n");
 		nvmet_tcp_alloc_queue(port, pdu.cmd.hdr.queue_id);
 	}
 	queue = port->queue_arr[pdu.cmd.hdr.queue_id];
-- 
2.37.2


From 19b812b5e862c610d44f8a233c32905df52334a1 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Fri, 17 Nov 2023 17:21:30 +0000
Subject: [PATCH 18/26] Better Debug Messages

---
 drivers/nvme/target/tcp.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 47ded8145f03..7053e678a0e5 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -895,6 +895,7 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 	pr_debug("Num of sg pages is : %d\n", cmd->req.sg_cnt);
 	struct bio_vec *data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
 	, GFP_KERNEL);
+	pr_debug("Sending CID: %d\n", cmd->req.cmd->common.command_id);
 
 	if (cmd->state == NVMET_TCP_SEND_DATA_PDU) {
 		pr_debug("Sending data pdu \n");
-- 
2.37.2


From 21abd53bf5427070c5627de63eee47e9b702910f Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 30 Nov 2023 19:33:04 +0000
Subject: [PATCH 19/26] Added debug messages to keep track of RPC

---
 drivers/nvme/target/tcp.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 7053e678a0e5..38d17fa0cffb 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -725,6 +725,7 @@ static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
 		memset(&msg, 0, sizeof(msg));
 		struct homa_sendmsg_args homa_args;
 		memset(&homa_args, 0, sizeof(homa_args));
+		pr_debug("Responding with RPC id : %lld\n", cmd->response_state.response_id);
 		homa_args.id = cmd->response_state.response_id;
 		msg.msg_name = &(cmd->response_state.source_addr);
 		msg.msg_namelen = sizeof(cmd->response_state.source_addr);
@@ -765,6 +766,7 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 	int ret;
 	struct homa_sendmsg_args homa_send_args;
 	memset(&homa_send_args, 0, sizeof(homa_send_args));
+	pr_debug("Responding with RPC id : %lld\n", cmd->response_state.response_id);
 	homa_send_args.id  = cmd->response_state.response_id;
 
 	msg.msg_name = &(cmd->response_state.source_addr);
@@ -823,6 +825,7 @@ static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
 	struct homa_sendmsg_args homa_send_args;
 	memset(&homa_send_args, 0, sizeof(homa_send_args));
+	pr_debug("Sending R2T with RPC id : %lld\n", cmd->response_state.response_id);
 	homa_send_args.id  = cmd->response_state.response_id;
 
 	msg.msg_name = &(cmd->response_state.source_addr);
@@ -1963,6 +1966,7 @@ static void nvmet_homa_read_sock(struct work_struct *w){
 		goto done;
 	}
 	pr_debug("length of message received : %d\n", len);
+	pr_debug("Recieved message for rpc id: %lld\n", homa_recv_args->id);
 	len = (int) sizeof(struct nvme_tcp_hdr);
 
 	memcpy(&pdu,port->homa_buf_args.start + homa_recv_args->bpage_offsets[0], len);
-- 
2.37.2


From d31fb598e3cda38742fa682d418f437081609229 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Wed, 3 Jan 2024 13:36:52 +0000
Subject: [PATCH 20/26] Increase Homa BufPool Size

---
 drivers/nvme/target/tcp.c | 10 +++++-----
 1 file changed, 5 insertions(+), 5 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 38d17fa0cffb..7ee9e6169815 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -2014,14 +2014,14 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 {
 	struct nvmet_tcp_port *port;
 	__kernel_sa_family_t af;
-	int bufsize = 64 * HOMA_BPAGE_SIZE;
+	int bufsize = 500 * HOMA_BPAGE_SIZE;
 	int ret;
 
 
 	port = kzalloc(sizeof(*port), GFP_KERNEL);
 	if (!port)
 		return -ENOMEM;
-	char *server_homa_buf_region = kzalloc(bufsize,GFP_KERNEL);
+	char *server_homa_buf_region = vzalloc(bufsize);
 
 	switch (nport->disc_addr.adrfam) {
 	case NVMF_ADDR_FAMILY_IP4:
@@ -2099,9 +2099,9 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 
 err_sock:
 	sock_release(port->sock);
-	kfree(server_homa_buf_region);
+	kvfree(server_homa_buf_region);
 err_port:
-	kfree(server_homa_buf_region);
+	kvfree(server_homa_buf_region);
 	kfree(port);
 	return ret;
 }
@@ -2134,7 +2134,7 @@ static void nvmet_tcp_remove_port(struct nvmet_port *nport)
 	nvmet_tcp_destroy_port_queues(port);
 
 	sock_release(port->sock);
-	kfree(port->homa_buf_args.start);
+	kvfree(port->homa_buf_args.start);
 	kfree(port);
 }
 
-- 
2.37.2


From 18227db590a2782a79e6c5c97ff36c2aa29a5a6b Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Mon, 15 Jan 2024 00:07:34 +0000
Subject: [PATCH 21/26] Add Module Parameter to Control MaxH2CSize Add Useful
 Print Statement(s)

---
 drivers/nvme/target/tcp.c | 10 ++++++++--
 1 file changed, 8 insertions(+), 2 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 7ee9e6169815..25e2ac1ef713 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -53,9 +53,13 @@ static const struct kernel_param_ops set_param_ops = {
  * values that may be unique for some NIC implementations.
  */
 static int so_priority;
+static u32 maxh2cdata = 512000;
 device_param_cb(so_priority, &set_param_ops, &so_priority, 0644);
 MODULE_PARM_DESC(so_priority, "nvmet tcp socket optimize priority: Default 0");
 
+device_param_cb(maxh2cdata, &set_param_ops, &maxh2cdata, 0644);
+MODULE_PARM_DESC(maxh2cdata, "nvmet tcp maxh2cdata value");
+
 /* Define a time period (in usecs) that io_work() shall sample an activated
  * queue before determining it to be idle.  This optional module behavior
  * can enable NIC solutions that support socket optimized packet processing
@@ -1046,7 +1050,7 @@ static int nvmet_tcp_handle_icreq(struct nvmet_tcp_queue *queue)
 	icresp->hdr.pdo = 0;
 	icresp->hdr.plen = cpu_to_le32(icresp->hdr.hlen);
 	icresp->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);
-	icresp->maxdata = cpu_to_le32(0x400000); /* 16M arbitrary limit */
+	icresp->maxdata = cpu_to_le32(maxh2cdata); /* 16M arbitrary limit */
 	icresp->cpda = 0;
 	if (queue->hdr_digest)
 		icresp->digest |= NVME_TCP_HDR_DIGEST_ENABLE;
@@ -2048,8 +2052,10 @@ static int nvmet_tcp_add_port(struct nvmet_port *nport)
 	port->nport = nport;
 	//INIT_WORK(&port->accept_work, nvmet_tcp_accept_work);
 	INIT_WORK(&port->read_sock, nvmet_homa_read_sock);
-	if (port->nport->inline_data_size < 0)
+	if (port->nport->inline_data_size < 0){
+		pr_info("Inline data size was not set by the controller!");
 		port->nport->inline_data_size = NVMET_TCP_DEF_INLINE_DATA_SIZE;
+	}
 
 	ret = sock_create(port->addr.ss_family, SOCK_DGRAM,
 				IPPROTO_HOMA, &port->sock);
-- 
2.37.2


From e1bde67979215cbaf3a38efd2d2bcd1fb634d6bc Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Mon, 15 Jan 2024 00:09:29 +0000
Subject: [PATCH 22/26] Allow Target To Respond To A H2CData Packet With An R2T
 If Needed

---
 drivers/nvme/target/tcp.c | 7 ++++++-
 1 file changed, 6 insertions(+), 1 deletion(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 25e2ac1ef713..293cfef6b07b 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -1343,8 +1343,13 @@ static int nvmet_tcp_try_recv_data(struct homa_recvmsg_args *homa_recv_args, str
 		return 0;
 	}
 
-	if (cmd->rbytes_done == cmd->req.transfer_len)
+	if (cmd->rbytes_done == cmd->req.transfer_len) /*So if we don't reaceive all the data what 
+	happens, also maybe a check for the last flag*/
 		nvmet_tcp_execute_request(cmd);
+	else if (cmd->rbytes_done < cmd->req.transfer_len){
+		pr_debug("We have only received %u bytes of data out of %zu bytes!!\n", cmd->rbytes_done, cmd->req.transfer_len);
+		nvmet_tcp_queue_response(&queue->cmd->req);
+	}
 
 	nvmet_prepare_receive_pdu(queue);
 	return 0;
-- 
2.37.2


From 0d52d5041c33b871de0dec62f90b19f566795138 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 18 Jan 2024 23:22:49 +0000
Subject: [PATCH 23/26] Fix Homa Copy Bug

---
 drivers/nvme/target/tcp.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 293cfef6b07b..6534eb5713a7 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -244,6 +244,7 @@ static int copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, int len
 		len_to_copy);
 		//ohhhhh, wait data_offset should only appy for the first page, after that we should start from 0. Sheeeeeeee.
 		memcpy(copy_to+ *offset, queue->port->homa_buf_args.start+ homa_recv_args->bpage_offsets[i] + data_offset, len);
+		data_offset = 0;
 		*offset += len;
 		len_to_copy -= len;
 	}
@@ -277,6 +278,7 @@ static int iter_copy_from_homa_pool(struct homa_recvmsg_args *homa_recv_args, in
 	HOMA_BPAGE_SIZE :
 		len_to_copy);
 		copy_to_iter(queue->port->homa_buf_args.start+ homa_recv_args->bpage_offsets[i] + data_offset, len, iter);
+		data_offset=0;
 		*offset += len;
 		len_to_copy -= len;
 	}
-- 
2.37.2


From ac6c0fc6c34c71ee2959f195ad7c8f319dd3b332 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 18 Jan 2024 23:33:10 +0000
Subject: [PATCH 24/26] Re-Architect Send Path

---
 drivers/nvme/target/tcp.c | 226 ++++++++++++++++++--------------------
 1 file changed, 107 insertions(+), 119 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 6534eb5713a7..37107aca2f87 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -215,6 +215,7 @@ static struct workqueue_struct *nvmet_tcp_wq;
 static const struct nvmet_fabrics_ops nvmet_tcp_ops;
 static void nvmet_tcp_free_cmd(struct nvmet_tcp_cmd *c);
 static void nvmet_tcp_free_cmd_buffers(struct nvmet_tcp_cmd *cmd);
+static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs);
 
 /* copy_from_homa_pool - copy data from the homa buffer pool to another buffer
    @homa_recv_args: The homa_recvmsg_args struct that contains the information of our message in the
@@ -674,95 +675,96 @@ static void nvmet_tcp_execute_request(struct nvmet_tcp_cmd *cmd)
 		cmd->req.execute(&cmd->req);
 }
 
-static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvec)
+static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
 {
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
+	int ret;
+	struct bio_vec *data_bvecs;
+
+	pr_debug("Num of sg pages is : %d\n", cmd->req.sg_cnt);
+	data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
+	, GFP_KERNEL);
+
+	bvec_set_virt(&data_bvecs[0], (void *)cmd->data_pdu + cmd->offset, left);
+	pr_debug("Length of data pdu: %d\n", data_bvecs[0].bv_len);
 
-	bvec_set_virt(bvec, (void *)cmd->data_pdu + cmd->offset, left);
-	pr_debug("Length of data pdu: %d\n", bvec->bv_len);
-	/*
-	do we even need this anymore?	
-		cmd->offset += ret;
-	left -= ret;
 
-	if (left)
-		return -EAGAIN;
-	*/
 	cmd->state = NVMET_TCP_SEND_DATA;
 	cmd->offset  = 0;
-	// do we even need this return?
-	return 1;
+	ret = nvmet_try_send_data(cmd, data_bvecs);
+
+	kfree(data_bvecs);
+	return ret;
+
 }
 
-static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs, bool last_in_batch)
+static int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs)
 {
 	struct nvmet_tcp_queue *queue = cmd->queue;
 	int ret;
-	int num_sgl_entries = cmd->req.sg_cnt;
 	int n = 1;
 	int send_len = bvecs[0].bv_len + cmd->req.transfer_len;
 	struct msghdr msg;
-	pr_debug("Total number of sgl entries: %d\n", num_sgl_entries);
+	memset(&msg, 0, sizeof(msg));
+	struct homa_sendmsg_args homa_args;
+	memset(&homa_args, 0, sizeof(homa_args));
 	pr_debug("Total number of bytes to send: %d\n", send_len);
 
-	while (cmd->cur_sg && n<=num_sgl_entries) {
+
+	struct bio_vec bvec;
+	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
+	int left; 
+
+	while (cmd->cur_sg && n<=cmd->req.sg_cnt) {
 		struct page *page = sg_page(cmd->cur_sg);
 		struct bio_vec bvec;
 		u32 left = cmd->cur_sg->length;
 
-		/*if ((!last_in_batch && cmd->queue->send_list_len) ||
-		    cmd->wbytes_done + left < cmd->req.transfer_len ||
-		    queue->data_digest || !queue->nvme_sq.sqhd_disabled)
-			msg.msg_flags |= MSG_MORE;*/
-
 		bvec_set_page(&bvec, page, left, cmd->offset);
 		bvecs[n] = bvec;
 
-		/* I don't think we need to care about cmd->offset
-		as it only comes into play with a sendmsg that doesn't
-		send all the data*/	
-		//cmd->offset += left;
 		cmd->wbytes_done += left;
 		cmd->cur_sg = sg_next(cmd->cur_sg);
 		n++;
 	}
 	if (queue->nvme_sq.sqhd_disabled) {
-		memset(&msg, 0, sizeof(msg));
-		struct homa_sendmsg_args homa_args;
-		memset(&homa_args, 0, sizeof(homa_args));
-		pr_debug("Responding with RPC id : %lld\n", cmd->response_state.response_id);
-		homa_args.id = cmd->response_state.response_id;
-		msg.msg_name = &(cmd->response_state.source_addr);
-		msg.msg_namelen = sizeof(cmd->response_state.source_addr);
-		msg.msg_control = &homa_args;
-		msg.msg_controllen = sizeof(homa_args);
-		msg.msg_control_is_user = false;
-
-		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, num_sgl_entries + 1, send_len);
-		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
-		pr_debug("Send data sock_sendmsg returned: %d\n", ret);
-		if (ret < 0){
-			return ret;
-		}
-		cmd->queue->snd_cmd = NULL;
-		nvmet_tcp_put_cmd(cmd);
-		nvmet_tcp_free_cmd_buffers(cmd);
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 1, send_len);
 	} else {
 		nvmet_setup_response_pdu(cmd);
+		left = sizeof(*cmd->rsp_pdu) - cmd->offset + hdgst;
+		bvec_set_virt(&bvecs[cmd->req.sg_cnt+1], (void *)cmd->rsp_pdu + cmd->offset, left);
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 2,
+		bvecs[0].bv_len + cmd->req.transfer_len + left);
 	}
+	pr_debug("Responding with RPC id : %lld\n", cmd->response_state.response_id);
+	homa_args.id = cmd->response_state.response_id;
+	msg.msg_name = &(cmd->response_state.source_addr);
+	msg.msg_namelen = sizeof(cmd->response_state.source_addr);
+	msg.msg_control = &homa_args;
+	msg.msg_controllen = sizeof(homa_args);
+	msg.msg_control_is_user = false;
 
-	if (queue->data_digest) {
-		cmd->state = NVMET_TCP_SEND_DDGST;
-		cmd->offset = 0;
+	ret = sock_sendmsg(cmd->queue->port->sock, &msg);
+	pr_debug("Send data sock_sendmsg returned: %d\n", ret);
+	if (ret < 0){
+		return ret;
 	}
+	cmd->queue->snd_cmd = NULL;
+	nvmet_tcp_put_cmd(cmd);
+	nvmet_tcp_free_cmd_buffers(cmd);
+
+
+	//if (queue->data_digest) {
+	//	cmd->state = NVMET_TCP_SEND_DDGST;
+	//	cmd->offset = 0;
+	//}
 
 	return 1;
 
 }
 
-static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bvecs,
-		bool last_in_batch)
+static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd)
 {
 	struct msghdr msg;
 	memset(&msg, 0, sizeof(msg));
@@ -781,52 +783,26 @@ static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd, struct bio_vec *bv
 	msg.msg_controllen = sizeof(homa_send_args);
 	msg.msg_control_is_user = false;
 
-	if(cmd->wbytes_done){
-		bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
-		bvecs[cmd->req.sg_cnt +1] = bvec;
-		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, bvecs, cmd->req.sg_cnt + 2,
-		bvecs[0].bv_len + cmd->req.transfer_len + left);
-		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
-		if (ret < 0)
-			return ret;
-	}
-	else{
-		bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
-		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
-		ret = sock_sendmsg(cmd->queue->port->sock, &msg);
-		if (ret < 0)
-			return ret;
-
-	}
-	cmd->offset += left;
-	left -= left;
-
-	/*if (left)
-		return -EAGAIN;
-		*/
-
+	bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
+	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+	ret = sock_sendmsg(cmd->queue->port->sock, &msg);
+	if (ret < 0)
+		return ret;
 	nvmet_tcp_free_cmd_buffers(cmd);
 	cmd->queue->snd_cmd = NULL;
 	nvmet_tcp_put_cmd(cmd);
 	return 1;
 }
 
-static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
+static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd)
 {
 	pr_debug("We in send_r2t!");
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct msghdr msg;
 	struct bio_vec bvec;
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->r2t_pdu) - cmd->offset + hdgst;
 	int ret;
 
-	/*
-	if (!last_in_batch && cmd->queue->send_list_len)
-		msg.msg_flags |= MSG_MORE;
-	else
-		msg.msg_flags |= MSG_EOR;
-	*/
-
 	bvec_set_virt(&bvec, (void *)cmd->r2t_pdu + cmd->offset, left);
 	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
 	struct homa_sendmsg_args homa_send_args;
@@ -842,13 +818,6 @@ static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 	ret = sock_sendmsg(cmd->queue->port->sock, &msg);
 	if (ret < 0)
 		return ret;
-	cmd->offset += left;
-	left -= left;
-
- /*
-	if (left)
-		return -EAGAIN;
-	*/
 	cmd->queue->snd_cmd = NULL;
 	return 1;
 }
@@ -901,48 +870,67 @@ static int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,
 			return 0;
 		}
 	}
-	pr_debug("Num of sg pages is : %d\n", cmd->req.sg_cnt);
-	struct bio_vec *data_bvecs = kmalloc_array(cmd->req.sg_cnt + 2, sizeof(struct bio_vec)
-	, GFP_KERNEL);
 	pr_debug("Sending CID: %d\n", cmd->req.cmd->common.command_id);
+	//So we have 3 cases to deal with here:
 
-	if (cmd->state == NVMET_TCP_SEND_DATA_PDU) {
+	switch (cmd->state)
+	{
+	case NVMET_TCP_SEND_DATA_PDU:
 		pr_debug("Sending data pdu \n");
-		ret = nvmet_try_send_data_pdu(cmd, data_bvecs);
-		if (ret <= 0)
-			goto done_send;
-	}
-
-	if (cmd->state == NVMET_TCP_SEND_DATA) {
-		pr_debug("Sending data \n");
-		ret = nvmet_try_send_data(cmd, data_bvecs,last_in_batch);
-		if (ret <= 0)
+		ret = nvmet_try_send_data_pdu(cmd);
+		if (ret<=0)
 			goto done_send;
-	}
-
-	if (cmd->state == NVMET_TCP_SEND_DDGST) {
-		pr_debug("Sending ddgst \n");
-		ret = nvmet_try_send_ddgst(cmd, last_in_batch);
+		break;
+	case NVMET_TCP_SEND_R2T:
+		pr_debug("Sending r2t\n");
+		ret = nvmet_try_send_r2t(cmd);
 		if (ret <= 0)
 			goto done_send;
-	}
-
-	if (cmd->state == NVMET_TCP_SEND_R2T) {
-		ret = nvmet_try_send_r2t(cmd, last_in_batch);
+		break;
+	case NVMET_TCP_SEND_RESPONSE:
+		pr_debug("Sending response \n");
+		ret = nvmet_try_send_response(cmd);
 		if (ret <= 0)
 			goto done_send;
+		break;
+	
+	default:
+		goto done_send;
+		break;
 	}
 
-	if (cmd->state == NVMET_TCP_SEND_RESPONSE){
-		pr_debug("Sending response \n");
-		ret = nvmet_try_send_response(cmd, data_bvecs,last_in_batch);
-	}
+	//if (cmd->state == NVMET_TCP_SEND_DATA_PDU) {
+	//	pr_debug("Sending data pdu \n");
+	//	ret = nvmet_try_send_data_pdu(cmd, data_bvecs);
+	//}
+
+	//if (cmd->state == NVMET_TCP_SEND_DATA) {
+	//	pr_debug("Sending data \n");
+	//	ret = nvmet_try_send_data(cmd, data_bvecs);
+	//	if (ret <= 0)
+	//		goto done_send;
+	//}
+
+	//if (cmd->state == NVMET_TCP_SEND_DDGST) {
+	//	pr_debug("Sending ddgst \n");
+	//	ret = nvmet_try_send_ddgst(cmd, last_in_batch);
+	//	if (ret <= 0)
+	//		goto done_send;
+	//}
+
+	//if (cmd->state == NVMET_TCP_SEND_R2T) {
+	//	ret = nvmet_try_send_r2t(cmd, last_in_batch);
+	//	if (ret <= 0)
+	//		goto done_send;
+	//}
+
+	//if (cmd->state == NVMET_TCP_SEND_RESPONSE){
+	//	pr_debug("Sending response \n");
+	//	ret = nvmet_try_send_response(cmd, data_bvecs,last_in_batch);
+	//}
 
 done_send:
-	kfree(data_bvecs);
 	if (ret < 0) {
-		if (ret == -EAGAIN)
-			return 0;
 		return ret;
 	}
 
-- 
2.37.2


From eb028e929de5acb7cdcb0820e66004aa851ef696 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Thu, 18 Jan 2024 23:35:52 +0000
Subject: [PATCH 25/26] Reduce Compiler Warnings Add Some Comments Remove
 Redundant Code

---
 drivers/nvme/target/tcp.c | 120 +++++---------------------------------
 1 file changed, 16 insertions(+), 104 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 37107aca2f87..7630843fd6e2 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -1171,9 +1171,6 @@ static int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)
 		return 0;
 	}
 
-	if(nvme_is_fabrics(nvme_cmd)){
-		pr_debug("Command identifier is %d\n", nvme_cmd->fabrics.command_id);
-	}
 	pr_debug("Command identifier is %d\n", nvme_cmd->common.command_id);
 
 	queue->cmd = nvmet_tcp_get_cmd(queue);
@@ -1268,13 +1265,6 @@ struct homa_recvmsg_args *homa_recv_args, int *offset)
 
 	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
 
-	queue->left -= len;
-
-
-
-	//if (queue->offset == sizeof(struct nvme_tcp_hdr)) 
-	u8 hdgst = nvmet_tcp_hdgst_len(queue);
-
 	if (unlikely(!nvmet_tcp_pdu_valid(hdr->type))) {
 		pr_err("unexpected pdu type %d\n", hdr->type);
 		nvmet_tcp_fatal_error(queue);
@@ -1288,9 +1278,9 @@ struct homa_recvmsg_args *homa_recv_args, int *offset)
 	/*queue->left = hdr->hlen - queue->offset + hdgst;
 	goto recv;*/
 	
-	len = hdr->hlen - queue->offset; //VERIFY THiS DEFNITELY
+	len = hdr->hlen - queue->offset; //lets us read the rest of the protocol specific header
+	//offset is reduced in homa_pool
 	copy_from_homa_pool(homa_recv_args, len , offset, queue, &queue->pdu);
-	queue->left -= len;
 
 	if (queue->hdr_digest &&
 	    nvmet_tcp_verify_hdgst(queue, &queue->pdu, hdr->hlen)) {
@@ -1432,7 +1422,6 @@ static int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)
 static int nvmet_tcp_try_recv(struct nvmet_tcp_queue *queue,
 		int budget, int *recvs)
 {
-	int i, ret = 0;
 	/*Some thoughts on this section:
 		- lol copilot don't know shit
 		- in the tcp model:
@@ -1815,49 +1804,6 @@ static void nvmet_tcp_state_change(struct sock *sk)
 	read_unlock_bh(&sk->sk_callback_lock);
 }
 
-static int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)
-{
-	struct socket *sock = queue->sock;
-	int ret;
-	//TODO: COME HERE AND CLEAR UP LARGE COMMENTED OUT BLOCKS
-	/*ret = kernel_getsockname(sock,
-		(struct sockaddr *)&queue->sockaddr);
-	if (ret < 0)
-		return ret;
-
-	ret = kernel_getpeername(sock,
-		(struct sockaddr *)&queue->sockaddr_peer);
-	if (ret < 0)
-		return ret;*/
-
-	/*
-	 * Cleanup whatever is sitting in the TCP transmit queue on socket
-	 * close. This is done to prevent stale data from being sent should
-	 * the network connection be restored before TCP times out.
-	 */
-	/*sock_no_linger(sock->sk);
-
-	if (so_priority > 0)
-		sock_set_priority(sock->sk, so_priority);
-
-	 Set socket type of service 
-	if (inet->rcv_tos > 0)
-		ip_sock_set_tos(sock->sk, inet->rcv_tos);*/
-
-	ret = 0;
-	write_lock_bh(&sock->sk->sk_callback_lock);
-	sock->sk->sk_user_data = queue;
-	queue->data_ready = sock->sk->sk_data_ready;
-	sock->sk->sk_data_ready = nvmet_tcp_data_ready;
-	queue->state_change = sock->sk->sk_state_change;
-	sock->sk->sk_state_change = nvmet_tcp_state_change;
-	queue->write_space = sock->sk->sk_write_space;
-	sock->sk->sk_write_space = nvmet_tcp_write_space;
-	//queue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);
-	write_unlock_bh(&sock->sk->sk_callback_lock);
-
-	return ret;
-}
 
 static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port, int queue_id)
 {
@@ -1897,18 +1843,22 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port, int queue_id)
 	nvmet_prepare_receive_pdu(queue);
 
 	mutex_lock(&nvmet_tcp_queue_mutex);
-	list_add_tail(&queue->queue_list, &nvmet_tcp_queue_list);
+	list_add_tail(&queue->queue_list, &nvmet_tcp_queue_list);/*We don't need this since we are 
+	keeping an array of queues?*/
 	mutex_unlock(&nvmet_tcp_queue_mutex);
 
 	if (idle_poll_period_usecs)
 		nvmet_tcp_arm_queue_deadline(queue);
 
 	return 0;
-out_destroy_sq:
-	mutex_lock(&nvmet_tcp_queue_mutex);
-	list_del_init(&queue->queue_list);
-	mutex_unlock(&nvmet_tcp_queue_mutex);
-	nvmet_sq_destroy(&queue->nvme_sq);
+
+/* this was for destroying queue when we couldn't set a socket for it in tcp could be useful to
+us in the future*/
+//out_destroy_sq:
+//	mutex_lock(&nvmet_tcp_queue_mutex);
+//	list_del_init(&queue->queue_list);
+//	mutex_unlock(&nvmet_tcp_queue_mutex);
+//	nvmet_sq_destroy(&queue->nvme_sq);
 out_free_connect:
 	nvmet_tcp_free_cmd(&queue->connect);
 out_ida_remove:
@@ -1918,27 +1868,6 @@ static int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port, int queue_id)
 	return ret;
 }
 
-static void nvmet_tcp_accept_work(struct work_struct *w)
-{
-	struct nvmet_tcp_port *port =
-		container_of(w, struct nvmet_tcp_port, accept_work);
-	struct socket *newsock;
-	int ret;
-
-	while (true) {
-		ret = kernel_accept(port->sock, &newsock, O_NONBLOCK);
-		if (ret < 0) {
-			if (ret != -EAGAIN)
-				pr_warn("failed to accept err=%d\n", ret);
-			return;
-		}
-		//ret = nvmet_tcp_alloc_queue(port, newsock);
-		if (ret) {
-			pr_err("failed to allocate queue\n");
-			sock_release(newsock);
-		}
-	}
-}
 static void nvmet_homa_read_sock(struct work_struct *w){
 	//This function can't be prempted by another read_sock
 	//As they it would only be queued after this one is done.
@@ -1947,7 +1876,7 @@ static void nvmet_homa_read_sock(struct work_struct *w){
 		container_of(w, struct nvmet_tcp_port, read_sock);
 	int len;
 	sockaddr_in_union source;
-	struct msghdr msg = { .msg_flags = MSG_DONTWAIT };
+	struct msghdr msg;
 	struct homa_recvmsg_args *homa_recv_args = &port->homa_recv_args;
 
 	union nvme_tcp_pdu pdu;  
@@ -1958,9 +1887,9 @@ static void nvmet_homa_read_sock(struct work_struct *w){
 	msg.msg_controllen = sizeof(*homa_recv_args);
 	msg.msg_name = &(source);
 	msg.msg_namelen = sizeof(source);
-	len = sock_recvmsg(port->sock, &msg, msg.msg_flags);
-	if (unlikely(len < 0)){
-		pr_debug("sock_recvmsg error returned %d\n", len);
+	len = sock_recvmsg(port->sock, &msg, 0);
+	if (len < 0 ){
+		//pr_debug("sock_recvmsg error returned %d\n", len);
 		//nvmet_tcp_socket_error(queue, len);
 		goto done;
 	}
@@ -1992,23 +1921,6 @@ static void nvmet_homa_read_sock(struct work_struct *w){
 
 }
 
-static void nvmet_tcp_listen_data_ready(struct sock *sk)
-{
-	struct nvmet_tcp_port *port;
-
-	trace_sk_data_ready(sk);
-
-	read_lock_bh(&sk->sk_callback_lock);
-	port = sk->sk_user_data;
-	if (!port)
-		goto out;
-
-	if (sk->sk_state == TCP_LISTEN)
-		queue_work(nvmet_wq, &port->accept_work);
-out:
-	read_unlock_bh(&sk->sk_callback_lock);
-}
-
 static int nvmet_tcp_add_port(struct nvmet_port *nport)
 {
 	struct nvmet_tcp_port *port;
-- 
2.37.2


From 90e5ed773af6f124ea7672232b259909dea41840 Mon Sep 17 00:00:00 2001
From: Suhas Narreddy <S.Narreddy@sms.ed.ac.uk>
Date: Sun, 21 Jan 2024 00:48:49 +0000
Subject: [PATCH 26/26] Queue IO Work On Queue 0

---
 drivers/nvme/target/tcp.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/drivers/nvme/target/tcp.c b/drivers/nvme/target/tcp.c
index 7630843fd6e2..0b87344fe1f4 100644
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -664,7 +664,7 @@ static void nvmet_tcp_queue_response(struct nvmet_req *req)
 
 	llist_add(&cmd->lentry, &queue->resp_list);
 	//here we have to think why we need to queue this as deferred work
-	queue_work(nvmet_tcp_wq, &cmd->queue->io_work);
+	queue_work_on(0,nvmet_tcp_wq, &cmd->queue->io_work);
 }
 
 static void nvmet_tcp_execute_request(struct nvmet_tcp_cmd *cmd)
@@ -1457,7 +1457,7 @@ static void nvmet_tcp_schedule_release_queue(struct nvmet_tcp_queue *queue)
 	spin_lock(&queue->state_lock);
 	if (queue->state != NVMET_TCP_Q_DISCONNECTING) {
 		queue->state = NVMET_TCP_Q_DISCONNECTING;
-		queue_work(nvmet_wq, &queue->release_work);
+		queue_work_on(0,nvmet_wq, &queue->release_work);
 	}
 	spin_unlock(&queue->state_lock);
 }
@@ -1744,7 +1744,7 @@ static void nvmet_tcp_data_ready(struct sock *sk)
 	read_unlock_bh(&sk->sk_callback_lock);
 
 	trace_sk_data_ready(sk);
-	queue_work( nvmet_tcp_wq, &port->read_sock);
+	queue_work_on(0, nvmet_tcp_wq, &port->read_sock);
 
 
 	/*
-- 
2.37.2

